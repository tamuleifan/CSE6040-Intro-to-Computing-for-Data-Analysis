{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b869117fb19fa7c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Logistic regression\n",
    "\n",
    "Beyond regression, another important data analysis task is _classification_, in which you are given a set of labeled data points and you wish to learn a model of the labels. The canonical example of a classification algorithm is _logistic regression_, the topic of this notebook.\n",
    "\n",
    "> Although it's called \"regression\" it is really a model for classification.\n",
    "\n",
    "Here, you'll consider _binary classification_. Each data point belongs to one of $c=2$ possible classes. By convention, we will denote these _class labels_ by \"0\" and \"1.\" However, the ideas can be generalized to the multiclass case, i.e., $c > 2$, with labels $\\{0, 1, \\ldots, c-1\\}$.\n",
    "\n",
    "You'll also want to review from earlier notebooks the concept of gradient ascent/descent (or \"steepest ascent/descent\"), when optimizing a scalar function of a vector variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2be32325da55a76b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 0: Introduction\n",
    "\n",
    "This part of the notebook introduces you to the classification problem through a \"geometric interpretation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6ec6a881ca6be880",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, Math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rc(\"savefig\", dpi=100) # Adjust for higher-resolution figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25bf58221e1d1389",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**A note about slicing columns from a Numpy matrix.** If you want to extract a column `i` from a Numpy matrix `A` _and_ keep it as a column vector, you need to use the slicing notation, `A[:, i:i+1]`. Not doing so can lead to subtle bugs. To see why, compare the following slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a6416d3e98bae48",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "A = np.array ([[1, 2, 3],\n",
    "               [4, 5, 6],\n",
    "               [7, 8, 9]\n",
    "              ], dtype=float)\n",
    "\n",
    "print (\"A[:, :] ==\\n\", A)\n",
    "print (\"\\na0 := A[:, 0] ==\\n\", A[:, 0])\n",
    "print (\"\\na1 := A[:, 2:3] == \\n\", A[:, 2:3])\n",
    "\n",
    "print (\"\\nAdd columns 0 and 2?\")\n",
    "a0 = A[:, 0]\n",
    "a1 = A[:, 2:3]\n",
    "print (a0 + a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9fc593942ce3c2e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Aside: Broadcasting in Numpy.** What is happening in the operation, `a0 + a1`, shown above? When the shapes of two objects do not match, Numpy tries to figure out if there is a natural way to make them compatible. See [this supplemental notebook](./mo_numpy_mo_problems.ipynb) for information on Numpy's \"broadcasting rule,\" along with other Numpy tips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-536ee18dfc3dfd40",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Example data: Rock lobsters!\n",
    "\n",
    "As a concrete example of a classification task, consider the results of [the following experiment](http://www.stat.ufl.edu/~winner/data/lobster_survive.txt).\n",
    "\n",
    "Some marine biologists started with a bunch of lobsters of varying sizes (size being a proxy for the stage of a lobster's development). They then tethered and exposed these lobsters to a variety of predators. Finally, the outcome that they measured is whether the lobsters survived or not.\n",
    "\n",
    "The data is a set of points, one point per lobster, where there is a single predictor (the lobster's size) and the response is whether the lobsters survived (label \"1\") or died (label \"0\").\n",
    "\n",
    "> For the original paper, see [this link](http://downeastinstitute.org/assets/files/Published%20papers/Wilkinson%20et%20al%202015-1.pdf). For what we can only guess is what marine biologists do in their labs, see [this image](http://i.imgur.com/dQDKgys.jpg) (or this [possibly not-safe-for-work alternative](http://web.archive.org/web/20120628012654/http://www.traemcneely.com/wp-content/uploads/2012/04/wpid-Lobster-Fights-e1335308484734.jpeg))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01dd5d9514695a44",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Start by downloading this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4869bfc2ce048825",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import hashlib\n",
    "import io\n",
    "\n",
    "def on_vocareum():\n",
    "    return os.path.exists('.voc')\n",
    "\n",
    "def download(file, local_dir=\"\", url_base=None, checksum=None):\n",
    "    local_file = \"{}{}\".format(local_dir, file)\n",
    "    if not os.path.exists(local_file):\n",
    "        if url_base is None:\n",
    "            url_base = \"https://cse6040.gatech.edu/datasets/\"\n",
    "        url = \"{}{}\".format(url_base, file)\n",
    "        print(\"Downloading: {} ...\".format(url))\n",
    "        r = requests.get(url)\n",
    "        with open(local_file, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "            \n",
    "    if checksum is not None:\n",
    "        with io.open(local_file, 'rb') as f:\n",
    "            body = f.read()\n",
    "            body_checksum = hashlib.md5(body).hexdigest()\n",
    "            assert body_checksum == checksum, \\\n",
    "                \"Downloaded file '{}' has incorrect checksum: '{}' instead of '{}'\".format(local_file,\n",
    "                                                                                           body_checksum,\n",
    "                                                                                           checksum)\n",
    "    print(\"'{}' is ready!\".format(file))\n",
    "    \n",
    "if on_vocareum():\n",
    "    URL_BASE = \"https://cse6040.gatech.edu/datasets/rock-lobster/\"\n",
    "    DATA_PATH = \"../resource/lib/publicdata/\"\n",
    "else:\n",
    "    URL_BASE = \"https://github.com/cse6040/labs-fa17/raw/master/datasets/rock-lobster/\"\n",
    "    DATA_PATH = \"\"\n",
    "\n",
    "datasets = {'lobster_survive.dat': '12fc1c22ed9b4d7bf04bf7e0fec996b7',\n",
    "            'logreg_points_train.csv': '25bbca6105bae047ac4d62ee8b76c841',\n",
    "            'log_likelihood_soln.npz': '5a9e17d56937855727afa6db1cd83306',\n",
    "            'grad_log_likelihood_soln.npz': 'a67c00bfa95929e12d423105d8412026',\n",
    "            'hess_log_likelihood_soln.npz': 'b46443fbf0577423b084122503125887'}\n",
    "\n",
    "for filename, checksum in datasets.items():\n",
    "    download(filename, local_dir=DATA_PATH, url_base=URL_BASE, checksum=checksum)\n",
    "    \n",
    "print(\"\\n(All data appears to be ready.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4b8ecbcdbed6ed0f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Here is a plot of the raw data, which was taken from [this source](http://www.stat.ufl.edu/~winner/data/lobster_survive.dat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34dbbf85a3b35b96",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_lobsters = pd.read_table('{}lobster_survive.dat'.format(DATA_PATH),\n",
    "                            sep=r'\\s+', names=['CarapaceLen', 'Survived'])\n",
    "display(df_lobsters.head())\n",
    "print(\"...\")\n",
    "display(df_lobsters.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b0479f3da84b12a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x=\"Survived\", y=\"CarapaceLen\",\n",
    "                    data=df_lobsters, inner=\"quart\")\n",
    "ax.set(xlabel=\"Survived? (0=no, 1=yes)\",\n",
    "       ylabel=\"\",\n",
    "       title=\"Body length (carpace, in mm) vs. survival\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-762ee83333ceac4c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Although the classes are distinct in the aggregate, where the median carapace (outer shell) length is around 36 mm for the lobsters that died and 42 mm for those that survived, they are not cleanly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e54df7bec0815e1b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Notation\n",
    "\n",
    "To develop some intuition and a classification algorithm, let's formulate the general problem and apply it to synthetic data sets.\n",
    "\n",
    "Let the data consist of $m$ observations of $d$ continuously-valued predictors. In addition, for each data observation we observe a binary label whose value is either 0 or 1.\n",
    "\n",
    "Just like our convention in the linear regression case, represent each observation, or data point, by an _augumented_ vector, $\\hat{x}_i^T$,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\hat{x}_i^T\n",
    "    & \\equiv &\n",
    "      \\left(\\begin{array}{ccccc}\n",
    "        x_{i,0} &\n",
    "        x_{i,1} &\n",
    "         \\cdots &\n",
    "        x_{i,d-1} &\n",
    "            1\n",
    "      \\end{array}\\right)\n",
    "      .\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is, the point is the $d$ coordinates augmented by an initial dummy coordinate whose value is 1. This convention is similar to what we did in linear regression.\n",
    "\n",
    "We can also stack these points as rows of a matrix, $X$, again, just as we did in regression:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  X \\equiv\n",
    "    \\left(\\begin{array}{c}\n",
    "      \\hat{x}_0^T \\\\\n",
    "      \\hat{x}_1^T \\\\\n",
    "      \\vdots \\\\\n",
    "      \\hat{x}_{m-1}^T\n",
    "    \\end{array}\\right)\n",
    "  & = &\n",
    "    \\left(\\begin{array}{ccccc}\n",
    "      x_{0,1} & x_{0,2} & \\cdots & x_{0,d} & 1 \\\\\n",
    "      x_{1,1} & x_{1,2} & \\cdots & x_{1,d} & 1 \\\\\n",
    "              &         & \\vdots & & 1\\\\\n",
    "      x_{m-1,1} & x_{m-1,2} & \\cdots & x_{m-1,d} & 1 \\\\\n",
    "    \\end{array}\\right).\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We will take the labels to be a binary vector, $y^T \\equiv \\left(y_0, y_1, \\ldots, y_{m-1}\\right)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a7faf8dffab8d3dd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Example: A synthetic training set.** We've pre-generated a synethetic data set consisting of labeled data points. Let's download and inspect it, first as a table and then visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-be9597462a136386",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('{}logreg_points_train.csv'.format(DATA_PATH))\n",
    "\n",
    "display(df.head())\n",
    "print(\"...\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-95e60122f5bf2c37",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def make_scatter_plot(df, x=\"x_0\", y=\"x_1\", hue=\"label\",\n",
    "                      palette={0: \"red\", 1: \"olive\"},\n",
    "                      size=5):\n",
    "    sns.lmplot(x=x, y=y, hue=hue, data=df, palette=palette,\n",
    "               fit_reg=False)\n",
    "\n",
    "mpl.rc(\"savefig\", dpi=120) # Adjust for higher-resolution figures\n",
    "make_scatter_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dc030189de19fae2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Next, let's extract the coordinates as a Numpy matrix of `points` and the labels as a Numpy column vector `labels`. Mathematically, the `points` matrix corresponds to $X$ and the `labels` vector corresponds to $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-10defdecccd3eb4d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "points = np.insert(df.as_matrix (['x_0', 'x_1']), 2, 1.0, axis=1)\n",
    "labels = df.as_matrix(['label'])\n",
    "\n",
    "print (\"First and last 5 points:\\n\", '='*23, '\\n', points[:5], '\\n...\\n', points[-5:], '\\n')\n",
    "print (\"First and last 5 labels:\\n\", '='*23, '\\n', labels[:5], '\\n...\\n', labels[-5:], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a04efb5033be35f2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Linear discriminants and the heaviside function\n",
    "\n",
    "Suppose you think that the _boundary_ between the two clusters may be represented by a line. For the synthetic data example above, I hope you'll agree that such a model is not a terrible one.\n",
    "\n",
    "A linear boundary is also known as a _linear discriminant_. Any point $x$ on this line may be described by $\\theta^T \\! x$, where $\\theta$ is a vector of coefficients:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\theta\n",
    "    & \\equiv &\n",
    "      \\left(\\begin{array}{c} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_d \\end{array}\\right)\n",
    "      .\n",
    "      \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For example, suppose our observations have two predictors each ($d=2$). Let the corresponding data point be $x^T \\equiv (x_0, x_1, x_2=1.0)$. Then, $\\theta^T \\! \\hat{x} = 0$ means that\n",
    "\n",
    "$$\n",
    "\\begin{array}{rrcl}\n",
    "  &\n",
    "  \\theta^T \\! x = 0\n",
    "  & = & \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 \\\\\n",
    "  \\implies\n",
    "  & x_1\n",
    "    & = & -\\frac{\\theta_2}{\\theta_1} - \\frac{\\theta_0}{\\theta_1} x_0.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7468579ce4fd7631",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "So that describes points _on_ the line. However, given _any_ point $x$ in the $d$-dimensional space that is _not_ on the line, $\\theta^T \\! x$ still produces a value: that value will be positive on one side of the line ($\\theta^T \\! x > 0$) or negative on the other ($\\theta^T \\! x < 0$).\n",
    "\n",
    "In other words, you can use the linear discriminant function, $\\theta^T \\! x$, to _generate_ a label for each point $x$: just reinterpret its sign!\n",
    "\n",
    "If you want \"0\" and \"1\" labels, the _heaviside function_, $H(y)$, will convert a positive $y$ to the label \"1\" and all other values to \"0.\"\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  H(y) & \\equiv & \\left\\{\\begin{array}{ll}\n",
    "      1 & \\mathrm{if}\\ y > 0\n",
    "      \\\\\n",
    "      0 & \\mathrm{if}\\ y \\leq 0\n",
    "    \\end{array}\\right..\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f1e3976c8ca5a8a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 0** (2 points). Given the a $m \\times (d+1)$ matrix of augmented points (i.e., the $X$ matrix) and a column vector $\\theta$ of length $d+1$, implement a function to compute the value of the linear discriminant at each point. That is, the function should return a (column) vector $y$ where the $y_i = \\theta^T \\! x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "lin_discr",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def lin_discr (X, theta):\n",
    "    ### BEGIN SOLUTION\n",
    "    return X.dot(theta)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "lin_discr__check",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `lin_discr__check`\n",
    "import random\n",
    "theta_test = [random.random() for _ in range (3)]\n",
    "x0_test = [random.random() for _ in range (2)]\n",
    "x1_test = [(-theta_test[2] - theta_test[0]*x0) / theta_test[1] for x0 in x0_test]\n",
    "X_test = np.array ([[x0*2 for x0 in x0_test] + [x0*0.5 for x0 in x0_test],\n",
    "                    x1_test + x1_test,\n",
    "                    [1.0, 1.0, 1.0, 1.0],]).T\n",
    "print(X_test, \"\\n\")\n",
    "LD_test = lin_discr(X_test, np.array([theta_test]).T)\n",
    "print (LD_test)\n",
    "assert (LD_test[:2] > 0).all ()\n",
    "assert (LD_test[2:] < 0).all ()\n",
    "print(\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c309993fc0f570a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 1** (2 points). Implement the _heaviside function_, $H(y)$. Your function should allow for an arbitrary _matrix_ of input values and should apply the heaviside function to each element. In the returned matrix, the elements should have a **floating-point type**.\n",
    "\n",
    "Example, the code snippet\n",
    "\n",
    "```python\n",
    "    A = np.array([[-0.5, 0.2, 0.0],\n",
    "                  [4.2, 3.14, -2.7]])\n",
    "    print(heaviside(A))\n",
    "```\n",
    "\n",
    "should display\n",
    "\n",
    "```\n",
    "    [[ 0.  1.  0.]\n",
    "     [ 1.  1.  0.]]\n",
    "```\n",
    "\n",
    "> There are several possible approaches that lead to one-line solutions. One uses only logical and arithmetic operators, which you will recall are implemented as elementwise operations for Numpy arrays. Another uses Numpy's [`sign()`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.sign.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "heaviside",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def heaviside(Y):\n",
    "    ### BEGIN SOLUTION\n",
    "    return 1.0*(Y > 0.0)\n",
    "\n",
    "    # Alternative solution:\n",
    "    #return (np.sign(Y) > 0) * 1.0\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "heaviside__check",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `heaviside__check`\n",
    "\n",
    "Y_test = np.array([[-2.3, 1.2, 7.],\n",
    "                   [0.0, -np.inf, np.inf]])\n",
    "H_Y_test = heaviside(Y_test)\n",
    "\n",
    "print(\"Y:\\n\", Y_test)\n",
    "print(\"\\nH(Y):\\n\", H_Y_test)\n",
    "\n",
    "assert (H_Y_test.astype(int) == np.array([[0, 1, 1], [0, 0, 1]])).all ()\n",
    "\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-be01e03df6f06be7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For the next exercise, we'll need the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-98ed742a9b73f69b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def heaviside_int(Y):\n",
    "    \"\"\"Evaluates the heaviside function, but returns integer values.\"\"\"\n",
    "    return heaviside(Y).astype(dtype=int)\n",
    "\n",
    "def gen_lin_discr_labels(points, theta, fun=heaviside_int):\n",
    "    \"\"\"\n",
    "    Given a set of points and the coefficients of a linear\n",
    "    discriminant, this function returns a set of labels for\n",
    "    the points with respect to this discriminant.\n",
    "    \"\"\"\n",
    "    score = lin_discr(points, theta)\n",
    "    labels = fun(score)\n",
    "    return labels\n",
    "\n",
    "def plot_lin_discr(theta, df, x=\"x_0\", y=\"x_1\", hue=\"label\",\n",
    "                   palette={0: \"red\", 1: \"olive\"}, size=5,\n",
    "                   linewidth=2):\n",
    "    lm = sns.lmplot(x=x, y=y, hue=hue, data=df, palette=palette,\n",
    "                    size=size, fit_reg=False)\n",
    "    \n",
    "    x_min, x_max = df[x].min(), df[x].max()\n",
    "    y_min, y_max = df[y].min(), df[y].max()\n",
    "    \n",
    "    x1_min = (-theta[2][0] - theta[0][0]*x_min) / theta[1][0]\n",
    "    x1_max = (-theta[2][0] - theta[0][0]*x_max) / theta[1][0]\n",
    "    plt.plot([x_min, x_max], [x1_min, x1_max], linewidth=linewidth)\n",
    "    \n",
    "    def expand_interval(x_limits, percent=10.0):\n",
    "        x_min, x_max = x_limits[0], x_limits[1]\n",
    "        if x_min < 0:\n",
    "            x_min *= 1.0 + 1e-2*percent\n",
    "        else:\n",
    "            x_min *= 1.0 - 1e-2*percent\n",
    "        if x_max > 0:\n",
    "            x_max *= 1.0 + 1e-2*percent\n",
    "        else:\n",
    "            x_max *= 1.0 + 1e-2*percent\n",
    "        return (x_min, x_max)\n",
    "    x_view = expand_interval((x_min, x_max))\n",
    "    y_view = expand_interval((y_min, y_max))\n",
    "    lm.axes[0,0].set_xlim(x_view[0], x_view[1])\n",
    "    lm.axes[0,0].set_ylim(y_view[0], y_view[1])\n",
    "    \n",
    "def mark_matches(a, b, exact=False):\n",
    "    \"\"\"\n",
    "    Given two Numpy arrays of {0, 1} labels, returns a new boolean\n",
    "    array indicating at which locations the input arrays have the\n",
    "    same label (i.e., the corresponding entry is True).\n",
    "    \n",
    "    This function can consider \"inexact\" matches. That is, if `exact`\n",
    "    is False, then the function will assume the {0, 1} labels may be\n",
    "    regarded as the same up to a swapping of the labels. This feature\n",
    "    allows\n",
    "    \n",
    "      a == [0, 0, 1, 1, 0, 1, 1]\n",
    "      b == [1, 1, 0, 0, 1, 0, 0]\n",
    "      \n",
    "    to be regarded as equal. (That is, use `exact=False` when you\n",
    "    only care about \"relative\" labeling.)\n",
    "    \"\"\"\n",
    "    assert a.shape == b.shape\n",
    "    a_int = a.astype(dtype=int)\n",
    "    b_int = b.astype(dtype=int)\n",
    "    all_axes = tuple(range(len(a.shape)))\n",
    "    assert ((a_int == 0) | (a_int == 1)).all()\n",
    "    assert ((b_int == 0) | (b_int == 1)).all()\n",
    "    \n",
    "    exact_matches = (a_int == b_int)\n",
    "    if exact:\n",
    "        return exact_matches\n",
    "\n",
    "    assert exact == False\n",
    "    num_exact_matches = np.sum(exact_matches)\n",
    "    if (2*num_exact_matches) >= np.prod(a.shape):\n",
    "        return exact_matches\n",
    "    return exact_matches == False # Invert\n",
    "    \n",
    "def count_matches(a, b, exact=False):\n",
    "    \"\"\"\n",
    "    Given two sets of {0, 1} labels, returns the number of mismatches.\n",
    "    \n",
    "    This function can consider \"inexact\" matches. That is, if `exact`\n",
    "    is False, then the function will assume the {0, 1} labels may be\n",
    "    regarded as similar up to a swapping of the labels. This feature\n",
    "    allows\n",
    "    \n",
    "      a == [0, 0, 1, 1, 0, 1, 1]\n",
    "      b == [1, 1, 0, 0, 1, 0, 0]\n",
    "      \n",
    "    to be regarded as equal. (That is, use `exact=False` when you\n",
    "    only care about \"relative\" labeling.)\n",
    "    \"\"\"\n",
    "    matches = mark_matches(a, b, exact=exact)\n",
    "    return int(matches.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a76131ff2fd35f48",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 2** (2 points). For the synthetic data you loaded above, try by hand to find a value for $\\theta$ such that $H(\\theta^T x)$ \"best\" separates the two clusters. Store this $\\theta$ in a variable named `my_theta`, which should be a Numpy _column vector_. That is, define `my_theta` here using a line like:\n",
    "\n",
    "```python\n",
    "my_theta = np_col_vec([3., 0., -1.])\n",
    "```\n",
    "\n",
    "where `np_col_vec` is defined below and the list of values are your best guesses at discriminating coefficients. The test code will check that your solution makes no more than ten misclassifications.\n",
    "\n",
    "> Hint: We found a set of coefficients that commits just 5 errors for the 375 input points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "my_theta",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def np_col_vec (list_values):\n",
    "    \"\"\"Returns a Numpy column vector for the given list of scalar values.\"\"\"\n",
    "    return np.array ([list_values]).T\n",
    "\n",
    "# Redefine `my_theta` as instructed above to reduce the number of mismatches:\n",
    "my_theta = np_col_vec([-1., 3., 0.]) # 123 mismatches\n",
    "### BEGIN SOLUTION\n",
    "my_theta = np_col_vec([-6.5, -1., -1.35]) # 5 mismatches\n",
    "my_theta = np_col_vec([-2., -0.5, -0.55]) # 5 mismatches\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "my_theta__check",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Here are the labels generated by your discriminant:\n",
    "my_labels = gen_lin_discr_labels(points, my_theta)\n",
    "\n",
    "# Here is a visual check:\n",
    "num_mismatches = len(labels) - count_matches(labels, my_labels)\n",
    "print (\"Detected\", num_mismatches, \"out of\", len(labels), \"mismatches.\")\n",
    "\n",
    "df_matches = df.copy ()\n",
    "df_matches['label'] = mark_matches (my_labels, labels).astype (dtype=int)\n",
    "\n",
    "mpl.rc(\"savefig\", dpi=100) # Adjust for higher-resolution figures\n",
    "plot_lin_discr (my_theta, df_matches)\n",
    "\n",
    "assert num_mismatches <= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-79c193391bfa60e0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**How the heaviside divides the space.** The heaviside function, $H(\\theta^T x)$, enforces a sharp boundary between classes around the $\\theta^T x=0$ line. The following code produces a [contour plot](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.contourf.html) to show this effect: there will be a sharp dividing line between 0 and 1 values, with one set of values shown as a solid dark area and the remaining as a solid light-colored area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c249a5ebe750c791",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x0 = np.linspace(-2., +2., 100)\n",
    "x1 = np.linspace(-2., +2., 100)\n",
    "x0_grid, x1_grid = np.meshgrid(x0, x1)\n",
    "h_grid = heaviside(my_theta[2] + my_theta[0]*x0_grid + my_theta[1]*x1_grid)\n",
    "plt.contourf(x0, x1, h_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9e5281924908ba41",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 1: The logistic (or sigmoid) function as an alternative discriminant\n",
    "\n",
    "As the lobsters example suggests, real data are not likely to be cleanly separable, especially when the number of features we have at our disposal is relatively small.\n",
    "\n",
    "Since the labels are 0 or 1, you could look for a way to interpret labels as _probabilities_ rather than as hard (0 or 1) labels. One such function is the _logistic function_, also referred to as the _logit_ or _sigmoid_ function.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  G(y) & \\equiv & \\dfrac{1}{1 + e^{-y}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The logistic function takes any value in the range $(-\\infty, +\\infty)$ and produces a value in the range $(0, 1)$. Thus, given a value $y$, we can interpret $G(y)$ as a conditional probability that the label is 1 given $y$, i.e., $G(y) \\equiv \\mathrm{Pr}[\\mbox{label is }1 \\,|\\, y]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1560f56eb9ee1082",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 3** (2 points). Implement the logistic function. Inspect the resulting plot of $G(y)$ in 1-D and then the contour plot of $G(\\theta^T{x})$. Your function should accept a Numpy matrix of values, `Y`, and apply the sigmoid elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "logistic",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def logistic(Y):\n",
    "    ### BEGIN SOLUTION\n",
    "    return 1.0 / (1.0 + np.exp (-Y))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "# Plot your function for a 1-D input.\n",
    "y_values = np.linspace(-10, 10, 100)\n",
    "\n",
    "mpl.rc(\"savefig\", dpi=120) # Adjust for higher-resolution figures\n",
    "sns.set_style(\"darkgrid\")\n",
    "y_pos = y_values[y_values > 0]\n",
    "y_rem = y_values[y_values <= 0]\n",
    "plt.plot(y_rem, heaviside (y_rem), 'b')\n",
    "plt.plot(y_pos, heaviside (y_pos), 'b')\n",
    "plt.plot(y_values, logistic (y_values), 'r--')\n",
    "#sns.regplot (y_values, heaviside (y_values), fit_reg=False)\n",
    "#sns.regplot (y_values, logistic (y_values), fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "logistic__check",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `logistic__check`\n",
    "\n",
    "assert logistic(np.log(3)) == 0.75\n",
    "assert logistic(-np.log(3)) == 0.25\n",
    "\n",
    "g_grid = logistic(my_theta[2] + my_theta[0]*x0_grid + my_theta[1]*x1_grid)\n",
    "plt.contourf (x0, x1, g_grid)\n",
    "assert ((np.round(g_grid) - h_grid).astype(int) == 0).all()\n",
    "\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d0a06da857d4d26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 4** (_optional_; ungraded). Consider a set of 1-D points generated by a _mixture of Gaussians_. That is, suppose that there are two Gaussian distributions over the 1-dimensional variable, $x \\in (-\\infty, +\\infty)$, that have the _same_ variance ($\\sigma^2$) but _different_ means ($\\mu_0$ and $\\mu_1$). Show that the conditional probability of observing a point labeled \"1\" given $x$ may be written as,\n",
    "\n",
    "$$\\mathrm{Pr}\\left[l=1\\,|\\,x\\right]\n",
    "    \\propto \\dfrac{1}{1 + e^{-(\\theta_0 x + \\theta_1)}},$$\n",
    "\n",
    "for a suitable definition of $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "_Hints._ Since the points come from Gaussian distributions,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}\\left[x \\, | \\, l\\right]\n",
    "    & \\equiv & \\dfrac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x - \\mu_l)^2}{2 \\sigma^2}\\right).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "To rewrite $\\mathrm{Pr}\\left[l\\,|\\,x\\right]$ in terms of $\\mathrm{Pr}\\left[x \\, | \\, l\\right]$, recall _Bayes's rule (also: Bayes's theorem)_:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}[l=1\\,|\\,x]\n",
    "    & = &\n",
    "      \\dfrac{\\mathrm{Pr}[x\\,|\\,l=1] \\, \\mathrm{Pr}[l=1]}\n",
    "            {\\mathrm{Pr}[x]},\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where the denominator can be expanded as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}[x] & = & \\mathrm{Pr}[x\\,|\\,l=0] \\, \\mathrm{Pr}[l=0] + \\mathrm{Pr}[x\\,|\\,l=1] \\, \\mathrm{Pr}[l=1].\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "You may assume the prior probabilities of observing a 0 or 1 are given by $\\mathrm{Pr}[l=0] \\equiv p_0$ and $\\mathrm{Pr}[l=1] \\equiv p_1$.\n",
    "\n",
    "> The point of this derivation is to show you that the definition of the logistic function does not just arise out of thin air. It also hints that you might expect a final algorithm for logistic regression based on using $G(y)$ as the discriminant will work well when the classes are best explained as a mixture of Gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e09f6941ca40eebf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Generalizing to $d$-dimensions.** The preceding exercise can be generalized to $d$-dimensions. Let $\\theta$ and $x$ be $(d+1)$-dimensional points. Then,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}\\left[l=1\\,|\\,x\\right]\n",
    "    & \\propto & \\dfrac{1}{1 + \\exp \\left( -\\theta^T \\! x \\right)}.\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d9b7b5c49fe0bda3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 5** (_optional_; ungraded). Verify the following properties of the logistic function, $G(y)$.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcll}\n",
    "  G(y)\n",
    "    & = & \\frac{e^y}{e^y + 1}\n",
    "    & \\mathrm{(P1)} \\\\\n",
    "  G(-y)\n",
    "    & = & 1 - G(y)\n",
    "    & \\mathrm{(P2)} \\\\\n",
    "  \\dfrac{dG}{dy}\n",
    "    & = & G(y) G(-y)\n",
    "    & \\mathrm{(P3)} \\\\\n",
    "  {\\dfrac{d}{dy}} {\\left[ \\ln G(y) \\right]}\n",
    "    & = & G(-y)\n",
    "    & \\mathrm{(P4)} \\\\\n",
    "  {\\dfrac{d}{dy}} {\\ln \\left[ 1 - G(y) \\right]}\n",
    "    & = & -G(y)\n",
    "    & \\mathrm{(P5)}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "g_props",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Answers.** In all of the derivations below, we use the fact that $G(y) > 0$.\n",
    "\n",
    "_(P1)_. Multiply the numerator and denominator by $e^y$.\n",
    "\n",
    "_(P2)_. Start with the right-hand side, $1 - G(y)$, apply some algebra, and then apply (P1).\n",
    "\n",
    "$$\n",
    "  1 - G(y)\n",
    "  = \\dfrac{e^y + 1}\n",
    "          {e^y + 1}\n",
    "    - \\dfrac{e^y}\n",
    "            {e^y + 1}\n",
    "  = \\dfrac{1}{e^y + 1} \\cdot \\dfrac{e^{-y}}{e^{-y}}\n",
    "  = \\dfrac{e^{-y}}{e^{-y} + 1}\n",
    "  = G(-y).\n",
    "$$\n",
    "\n",
    "_(P3)_. By direct calculation and application of (P1):\n",
    "\n",
    "$$\n",
    "  \\dfrac{dG}{dy}\n",
    "  = \\dfrac{d}{dy}\\left( 1 + e^{-y} \\right)^{-1}\n",
    "  = - \\left( 1 + e^{-y} \\right)^{-2} \\cdot (-e^{-y})\n",
    "  = \\underbrace{\\dfrac{1}{1 + e^{-y}}}_{= G(y)} \\cdot \\underbrace{\\dfrac{e^{-y}}{1 + e^{-y}}}_{= G(-y)}\n",
    "  = G(y) \\cdot G(-y).\n",
    "$$\n",
    "\n",
    "_(P4)_. By the chain rule and application of (P3):\n",
    "\n",
    "$$\n",
    "  \\dfrac{d}{dy} \\ln G(y)\n",
    "  = \\left(\\dfrac{d}{dG} \\ln G\\right) \\dfrac{dG}{dy}\n",
    "  = \\dfrac{1}{G(y)} \\cdot G(y) G(-y)\n",
    "  = G(-y).\n",
    "$$\n",
    "\n",
    "_(P5)_. By combining (P2), variable substitution and the chain rule, and (P4),\n",
    "\n",
    "$$\n",
    "  \\dfrac{d}{dy} \\ln \\left[ 1 - G(y) \\right]\n",
    "  = \\dfrac{d}{dy} \\ln G(-y)\n",
    "  = \\underbrace{ \\left[ \\dfrac{d}{dz} \\ln G(z) \\right] \\cdot \\dfrac{dz}{dy} }_{ \\mbox{Let } z \\equiv -y}\n",
    "  = G(-z) \\cdot (-1)\n",
    "  = -G(y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-094c22dcbd2bd300",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Determining the discriminant via maximum likelihood estimation\n",
    "\n",
    "Previously, you determined $\\theta$ for our synthetic dataset by hand. Can you compute a good $\\theta$ automatically? One of the standard techniques in statistics is to perform a _maximum likelihood estimation_ (MLE) of a model's parameters, $\\theta$. Indeed, you may have seen or used MLE to derive the normal equations for linear regression in a more \"statistically principled\" way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a977bf3b62c2142",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**\"Likelihood\" as an objective function.** MLE derives from the following idea. Consider the joint probability of observing all of the labels, given the points and the parameters, $\\theta$:\n",
    "\n",
    "$$\n",
    "  \\mathrm{Pr}[y\\,|\\,X, \\theta].\n",
    "$$\n",
    "\n",
    "Suppose these observations are independent and identically distributed (i.i.d.). Then the joint probability can be factored as the product of individual probabilities,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}[y\\, | \\,X, \\theta] = \\mathrm{Pr}[y_0, \\ldots, y_{m-1} \\,|\\, \\hat{x}_0, \\ldots, \\hat{x}_{m-1}, \\theta]\n",
    "  & = & \\mathrm{Pr}[y_0 \\,|\\, \\hat{x}_0, \\theta] \\cdots \\mathrm{Pr}[y_{m-1} \\,|\\, \\hat{x}_{m-1}, \\theta] \\\\\n",
    "  & = & \\displaystyle \\prod_{i=0}^{m-1} \\mathrm{Pr}[y_i \\,|\\, \\hat{x}_i, \\theta].\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The _maximum likelihood principle_ says that you should choose $\\theta$ to maximize the chances (or \"likelihood\") of seeing these particular observations. Thus, $\\mathrm{Pr}[y\\, | \\,X, \\theta]$ is now an objective function to maximize.\n",
    "\n",
    "For both mathematical and numerical reasons, we will use the _logarithm_ of the likelihood, or _log-likelihood_, as the objective function instead. Let's define it as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathcal{L}(\\theta; y, X)\n",
    "    & \\equiv &\n",
    "      \\log \\left\\{ \\displaystyle \\prod_{i=0}^{m-1} \\mathrm{Pr}[y_i \\,|\\, \\hat{x}_i, \\theta] \\right\\} \\\\\n",
    "    & = &\n",
    "      \\displaystyle \\sum_{i=0}^{m-1} \\log \\mathrm{Pr}[y_i \\,|\\, \\hat{x}_i, \\theta].\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "> We are using the symbol $\\log$, which could be taken in any convenient base, such as the natural logarithm ($\\ln y$) or the information theoretic base-two logarithm ($\\log_2 y$).\n",
    "\n",
    "The MLE fitting procedure then consists of two steps:\n",
    "\n",
    "* For the problem at hand, decide on a model of $\\mathrm{Pr}[y_i \\,|\\, \\hat{x}_i, \\theta]$.\n",
    "* Run any optimization procedure to find the $\\theta$ that maximizes $\\mathcal{L}(\\theta; y, X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab3544c45593f9dd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 3: MLE for logistic regression\n",
    "\n",
    "Let's say you have decided that the logistic function, $G(\\hat{x}_i^T \\theta) = G(\\theta^T \\hat{x}_i)$, is a good model of the probability of producing a label $y_i$ given the observation $\\hat{x}_i^T$. Under the i.i.d. assumption, you can interpret the label $y_i$ as the result of flipping a coin, or a [Bernoulli trial](https://en.wikipedia.org/wiki/Bernoulli_trial), where the probability of success ($y_i=1$) is defined as $g_i = g_i(\\theta) \\equiv G(\\hat{x}_i^T \\theta)$. Thus,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}[y_i \\,|\\, \\hat{x}_i, \\theta]\n",
    "    & \\equiv & g_i^{y_i} \\cdot \\left(1 - g_i\\right)^{1 - y_i}.\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a33d8e67116a2150",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The log-likelihood in turn becomes,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathcal{L}(\\theta; y, X)\n",
    "    & = & \\displaystyle\n",
    "      \\sum_{i=0}^{m-1} y_i \\ln g_i + (1-y_i) \\ln (1-g_i) \\\\\n",
    "    & = & \\displaystyle\n",
    "      \\sum_{i=0}^{m-1} y_i \\ln \\dfrac{g_i}{1-g_i} + \\ln (1-g_i) \\\\\n",
    "    & = & \\displaystyle\n",
    "      \\sum_{i=0}^{m-1} y_i \\theta^T \\hat{x}_i + \\ln (1-g_i).\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9f279e7705721a77",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You can write the log-likelihood more compactly in the language of linear algebra.\n",
    "\n",
    "**Convention 1.** Let $u \\equiv (1, \\ldots, 1)^T$ be a column vector of all ones, with its length inferred from context. Let $A = \\left(\\begin{array}{cccc} a_0 & a_1 & \\cdots & a_{n-1} \\end{array}\\right)$ be any matrix, where $\\{a_i\\}$ denote its $n$ columns. Then, the sum of the columns is\n",
    "\n",
    "$$\\sum_{i=0}^{n-1} a_i\n",
    "  = \\left(a_0\\ a_1\\ \\cdots\\ a_{n-1}\\right)\n",
    "      \\cdot \\left(\\begin{array}{c}\n",
    "              1 \\\\\n",
    "              1 \\\\\n",
    "              \\vdots \\\\\n",
    "              1\n",
    "            \\end{array}\\right)\n",
    "  = A u.\n",
    "$$\n",
    "\n",
    "**Convention 2.** Let $A = \\left(a_{ij}\\right)$ be any matrix and let $f(y)$ be any function that we have defined by default to accept a scalar argument $y$ and produce a scalar result. For instance, $f(y) = \\ln y$ or $f(y) = G(y)$. Then, assume that $B = f(A)$ applies $f(\\cdot)$ elementwise to $A$, returning a matrix $B$ whose elements $b_{ij} = f(a_{ij})$.\n",
    "\n",
    "With these notational conventions, convince yourself that these are two different ways to write the log-likelihood for logistic regression.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rrcl}\n",
    "  (\\mathrm{V1}) & \\mathcal{L}(\\theta; y, X) & = & y^T \\ln G(X \\theta) + (u-y)^T \\ln [u - G(X \\theta)] \\\\\n",
    "  (\\mathrm{V2}) & \\mathcal{L}(\\theta; y, X) & = & y^T X \\theta + u^T \\ln G(-X \\theta)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8b00515ffcc60cd9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 6** (2 points). Implement the log-likelihood function in Python by defining a function with the following signature:\n",
    "\n",
    "```python\n",
    "  def log_likelihood (theta, y, X):\n",
    "    ...\n",
    "```\n",
    "\n",
    "> To compute the elementwise logarithm of a matrix or vector, use Numpy's [`log`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "log_likelihood",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood(theta, y, X):\n",
    "### BEGIN SOLUTION\n",
    "    u = np.ones((len (X), 1)) # column of all ones\n",
    "    z = X.dot(theta)\n",
    "    return y.T.dot(z) + u.T.dot(np.log(logistic(-z)))\n",
    "\n",
    "def log_likelihood_alt(theta, y, X):\n",
    "    z = X.dot(theta)\n",
    "    g = logistic(z)\n",
    "    return y.T.dot(np.log(g)) + (1.0-y).T.dot(np.log(1.0-g))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "log_likelihood__check",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `log_likelihood__check`\n",
    "\n",
    "if False:\n",
    "    d_soln = 10\n",
    "    m_soln = 1000\n",
    "    theta_soln = np.random.random ((d_soln+1, 1)) * 2.0 - 1.0\n",
    "    y_soln = np.random.randint (low=0, high=2, size=(m_soln, 1))\n",
    "    X_soln = np.random.random ((m_soln, d_soln+1)) * 2.0 - 1.0\n",
    "    X_soln[:, 0] = 1.0\n",
    "    L_soln = log_likelihood (theta_soln, y_soln, X_soln)\n",
    "    np.savez_compressed('{}log_likelihood_soln'.format(DATA_PATH),\n",
    "                        d_soln, m_soln, theta_soln, y_soln, X_soln, L_soln)\n",
    "\n",
    "npzfile_soln = np.load('{}log_likelihood_soln.npz'.format(DATA_PATH))\n",
    "d_soln = npzfile_soln['arr_0']\n",
    "m_soln = npzfile_soln['arr_1']\n",
    "theta_soln = npzfile_soln['arr_2']\n",
    "y_soln = npzfile_soln['arr_3']\n",
    "X_soln = npzfile_soln['arr_4']\n",
    "L_soln = npzfile_soln['arr_5']\n",
    "\n",
    "L_you = log_likelihood(theta_soln, y_soln, X_soln)\n",
    "your_err = np.max(np.abs(L_you/L_soln - 1.0))\n",
    "display(Math(r'\\left\\|\\dfrac{\\mathcal{L}_{\\tiny \\mbox{yours}} - \\mathcal{L}_{\\tiny \\mbox{solution}}}{\\mathcal{L}_{\\tiny \\mbox{solution}}}\\right\\|_\\infty \\approx %g' % your_err))\n",
    "assert your_err <= 1e-12\n",
    "\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-041c549288c30ba4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 4: Computing the MLE solution via gradient ascent: theory\n",
    "\n",
    "To optimize the log-likelihood with respect to the parameters, $\\theta$, you want to \"set the derivative to zero\" and solve for $\\theta$.\n",
    "\n",
    "For example, recall that in the case of linear regression via least squares minimization, carrying out this process produced an _analytic_ solution for the parameters, which was to solve the normal equations.\n",
    "\n",
    "Unfortunately, for logistic regression---or for most log-likelihoods you are likely to ever write down---you _cannot_ usually derive an analytic solution. Therefore, you will need to resort to numerical optimization procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-637576fcada09d2e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Gradient ascent, in 1-D.** A simple numerical algorithm to maximize a function is _gradient ascent_ (or _steepest ascent_). If instead you are minimizing the function, then the equivalent procedure is gradient (or steepest) _descent_. Here is the basic idea in 1-D.\n",
    "\n",
    "Suppose we wish to find the maximum of a scalar function $f(x)$ in one dimension. At the maximum, $\\dfrac{df(x)}{dx} = 0$.\n",
    "\n",
    "Suppose instead that $\\dfrac{df}{dx} \\neq 0$ and consider the value of $f$ at a nearby point, $x + s$, as given approximately by a truncated Taylor series:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  f(x + s)\n",
    "    & = &\n",
    "      f(x) + s \\dfrac{df(x)}{dx} + \\mathcal{O}(s^2).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "To make progress toward maximizing $f(x)$, you'd like to choose $s$ so that $f(x+s) > f(x)$. One way is to choose $s=\\alpha \\cdot \\mathrm{sign} \\left(\\dfrac{df}{dx}\\right)$, where $0 < \\alpha \\ll 1$ is \"small:\"\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  f \\left(x + \\alpha \\cdot \\mathrm{sign} \\left(\\dfrac{df}{dx}\\right) \\right)\n",
    "    & \\approx &\n",
    "      f(x) + \\alpha \\left|\\dfrac{df}{dx}\\right| + \\mathcal{O}(\\alpha^2).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "If $\\alpha$ is small enough, then you can neglect the $\\mathcal{O}(\\alpha^2)$ term and $f(x + s)$ will be larger than $f(x)$, thus making progress toward finding a maximum.\n",
    "\n",
    "This scheme is the basic idea: starting from some initial guess $x$, refine the guess by taking a small step $s$ _in the direction_ of the derivative, i.e., $\\mathrm{sign} \\left(\\dfrac{df}{dx}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c975de263048070b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Gradient ascent in higher dimensions.** Now suppose $x$ is a vector rather than a scalar. Then the value of $f$ at a nearby point $f(x + s)$, where $s$ is a _vector_, becomes\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  f(x + s) = f(x) + s^T \\nabla_x f(x) + \\mathcal{O}(\\|s\\|^2),\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $\\nabla_x f(x)$ is the gradient of $f$ with respect to $x$. As in the 1-D case, you want a step $s$ such that $f(x + s) > f(x)$. To make as much progress as possible, let's choose $s$ to be parallel to $\\nabla_x\\,f(x)$, that is, proportional to the gradient:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  s \\equiv \\alpha \\dfrac{\\nabla_x\\,f(x)}{\\|\\nabla_x\\,f(x)\\|}.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Again, $\\alpha$ is a fudge (or \"gentle nudge?\") factor. You need to choose it to be small enough that the high-order terms of the Taylor approximation become negligible, yet large enough that you can make reasonable progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-30f2982cf3b601e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**The gradient ascent procedure applied to MLE.** Applying gradient ascent to the problem of maximizing the log-likelihood leads to the following algorithm.\n",
    "\n",
    "* Start with some initial guess, $\\theta(0)$.\n",
    "* At each iteration $t \\geq 0$ of the procedure, let $\\theta(t)$ be the current guess.\n",
    "* Compute the direction of steepest ascent by evaluating the gradient, $\\Delta_t \\equiv \\nabla_{\\theta(t)} \\left\\{\\mathcal{L}(\\theta(t); y, X)\\right\\}$.\n",
    "* Define the step to be $s_t \\equiv \\alpha \\dfrac{\\Delta_t}{\\|\\Delta_t\\|}$, where $\\alpha$ is a suitably chosen fudge factor.\n",
    "* Take a step in the direction of the gradient, $\\theta(t+1) \\leftarrow \\theta(t) + s_t$.\n",
    "* Stop when the parameters don't change much _or_ after some maximum number of steps.\n",
    "\n",
    "This procedure should remind you of one you saw in a prior notebook (the least mean square algorithm for online regression!). As was true at that time, the tricky bit is how to choose $\\alpha$.\n",
    "\n",
    "> There is at least one difference between this procedure and the online regression procedure you learned earlier. Here, we are optimizing using the _full_ dataset rather than processing data points one at a time. (That is, the step iteration variable $t$ used above is not used in exactly the same way as the step iteration in LMS.)\n",
    ">\n",
    "> Another question is, how do we know this procedure will converge to the global maximum, rather than, say, a local maximum? For that you need a deeper analysis of a specific $\\mathcal{L}(\\theta; y, X)$, to show, for instance, that it is convex in $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-21891efe7610f21d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Implementing logistic regression using MLE by gradient ascent\n",
    "\n",
    "Let's apply the gradient ascent procedure to the logistic regression problem, in order to determine a good $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c8ee5e91241ce18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 7** (_optional_; ungraded). Show the following.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\nabla_\\theta \\left\\{\\mathcal{L}(\\theta; y, X)\\right\\}\n",
    "    & = & X^T \\left[ y - G(X \\cdot \\theta)\\right].\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "grad_log_likelihood_math",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Answer.** From (V2),\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}(\\theta; y, X) = y^T X \\theta + u^T \\ln G(-X \\theta).\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "  \\nabla_\\theta \\{\\mathcal{L}(\\theta; y, X)\\}\n",
    "  = \\nabla_\\theta (y^T X \\theta)\n",
    "    + \\nabla_\\theta \\left( u^T \\ln G(-X \\theta) \\right).\n",
    "$$\n",
    "\n",
    "Let's consider each term in turn.\n",
    "\n",
    "For the first term, apply the gradient identities to obtain\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\nabla_\\theta \\, (y^T X \\theta) & = & \\nabla_\\theta \\, (\\theta^T X^T y) = X^T y.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "For the second term, recall the scalar interpretation of $u^T \\ln G(-X \\theta)$.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  u^T \\ln G(-X \\theta)\n",
    "  & = & \\sum_{j=0}^{m-1} \\ln G\\left(-\\hat{x}_j^T \\theta\\right).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The $i$-th component of the gradient is\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\dfrac{\\partial}{\\partial \\theta_i} \\sum_{j=0}^{m-1} \\ln G\\left(-\\hat{x}_j^T \\theta\\right)\n",
    "  & = & \\sum_{j=0}^{m-1} \\dfrac{\\partial}{\\partial \\theta_i} \\ln G\\left(-\\hat{x}_j^T \\theta\\right).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Let's evaluate the summand:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial}{\\partial \\theta_i} \\ln G\\left(-\\hat{x}_j^T \\theta\\right)\n",
    "= \\underbrace{\\left[ \\dfrac{d}{dz} \\ln G(z) \\right] \\cdot \\left[ \\dfrac{\\partial z}{\\partial \\theta_i} \\right]}_{\\mbox{Let } z \\, \\equiv \\, -\\hat{x}_j^T \\theta}\n",
    "= G(-z) \\cdot \\dfrac{\\partial}{\\partial \\theta_i} \\left(-\\hat{x}_j^T \\theta\\right)\n",
    "= - G\\left(\\hat{x}_j^T \\theta\\right) \\cdot x_{ji}.\n",
    "$$\n",
    "\n",
    "Thus, the $i$-th component of the gradient becomes\n",
    "\n",
    "$$\n",
    "  \\left[\\nabla_\\theta \\left( u^T \\ln G(-X \\theta) \\right)\\right]_i\n",
    "  = - \\sum_{j=0}^{m-1} G\\left(\\hat{x}_j^T \\theta\\right) \\cdot x_{ji}.\n",
    "$$\n",
    "\n",
    "In other words, the full gradient vector is\n",
    "\n",
    "$$\n",
    "  \\nabla_\\theta \\left( u^T \\ln G(-X \\theta) \\right)\n",
    "  = -X^T G(X \\theta).\n",
    "$$\n",
    "\n",
    "Putting the two components together,\n",
    "\n",
    "$$\n",
    "  \\nabla_\\theta \\{\\mathcal{L}(\\theta; y, X)\\}\n",
    "  = X^T y - X^T G(X \\theta)\n",
    "  = X^T \\left[ y - G(X \\theta) \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-11d92e5e1f98bcd4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 8** (2 points). Implement a function to compute the gradient of the log-likelihood. Your function should have the signature,\n",
    "\n",
    "```python\n",
    "  def grad_log_likelihood (theta, y, X):\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "grad_log_likelihood_code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad_log_likelihood(theta, y, X):\n",
    "    \"\"\"Returns the gradient of the log-likelihood.\"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return X.T.dot(y - logistic(X.dot(theta)))\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "grad_log_likelihood_code__check",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `grad_log_likelihood_code__check`\n",
    "\n",
    "if False:\n",
    "    d_grad_soln = 6\n",
    "    m_grad_soln = 399\n",
    "    theta_grad_soln = np.random.random((d_grad_soln+1, 1)) * 2.0 - 1.0\n",
    "    y_grad_soln = np.random.randint(low=0, high=2, size=(m_grad_soln, 1))\n",
    "    X_grad_soln = np.random.random((m_grad_soln, d_grad_soln+1)) * 2.0 - 1.0\n",
    "    X_grad_soln[:, 0] = 1.0\n",
    "    L_grad_soln = grad_log_likelihood(theta_grad_soln, y_grad_soln, X_grad_soln)\n",
    "    np.savez_compressed('{}grad_log_likelihood_soln'.format(DATA_PATH),\n",
    "                        d_grad_soln, m_grad_soln, theta_grad_soln, y_grad_soln, X_grad_soln, L_grad_soln)\n",
    "\n",
    "npzfile_grad_soln = np.load ('{}grad_log_likelihood_soln.npz'.format(DATA_PATH))\n",
    "d_grad_soln = npzfile_grad_soln['arr_0']\n",
    "m_grad_soln = npzfile_grad_soln['arr_1']\n",
    "theta_grad_soln = npzfile_grad_soln['arr_2']\n",
    "y_grad_soln = npzfile_grad_soln['arr_3']\n",
    "X_grad_soln = npzfile_grad_soln['arr_4']\n",
    "L_grad_soln = npzfile_grad_soln['arr_5']\n",
    "\n",
    "L_grad_you = grad_log_likelihood (theta_grad_soln, y_grad_soln, X_grad_soln)\n",
    "your_grad_err = np.max (np.abs (L_grad_you/L_grad_soln - 1.0))\n",
    "display (Math (r'\\left\\|\\dfrac{\\nabla\\, \\mathcal{L}_{\\tiny \\mbox{yours}} - \\nabla\\,\\mathcal{L}_{\\tiny \\mbox{solution}}}{\\nabla\\, \\mathcal{L}_{\\tiny \\mbox{solution}}}\\right\\|_\\infty \\approx %g' % your_grad_err))\n",
    "assert your_grad_err <= 1e-12\n",
    "\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c750772abe501646",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 9** (4 points). Implement the gradient ascent procedure to determine $\\theta$, and try it out on the sample data.\n",
    "\n",
    "Recall the procedure (repeated from above):\n",
    "* Start with some initial guess, $\\theta(0)$.\n",
    "* At each iteration $t \\geq 0$ of the procedure, let $\\theta(t)$ be the current guess.\n",
    "* Compute the direction of steepest ascent by evaluating the gradient, $\\Delta_t \\equiv \\nabla_{\\theta(t)} \\left\\{\\mathcal{L}(\\theta(t); y, X)\\right\\}$.\n",
    "* Define the step to be $s_t \\equiv \\alpha \\dfrac{\\Delta_t}{\\|\\Delta_t\\|}$, where $\\alpha$ is a suitably chosen fudge factor.\n",
    "* Take a step in the direction of the gradient, $\\theta(t+1) \\leftarrow \\theta(t) + s_t$.\n",
    "* Stop when the parameters don't change much _or_ after some maximum number of steps.\n",
    "\n",
    "In the code skeleton below, we've set up a loop to run a fixed number, `MAX_STEP`, of gradient ascent steps. Also, when normalizing the step $\\Delta_t$, use the two-norm.\n",
    "\n",
    "> In your solution, we'd like you to store *all* guesses in the matrix `thetas`, so that you can later see how the $\\theta(t)$ values evolve. To extract a particular column `t`, use the notation, `theta[:, t:t+1]`. This notation is necessary to preserve the \"shape\" of the column as a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "logreg_mle",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "ALPHA = 0.1\n",
    "MAX_STEP = 250\n",
    "\n",
    "# Get the data coordinate matrix, X, and labels vector, y\n",
    "X = points\n",
    "y = labels.astype(dtype=float)\n",
    "\n",
    "# Store *all* guesses, for subsequent analysis\n",
    "thetas = np.zeros((3, MAX_STEP+1))\n",
    "\n",
    "for t in range(MAX_STEP):\n",
    "    # Fill in the code to compute thetas[:, t+1:t+2]\n",
    "    ### BEGIN SOLUTION\n",
    "    theta_t = thetas[:, t:t+1]\n",
    "    delta_t = grad_log_likelihood(theta_t, y, X)\n",
    "    delta_t = delta_t / np.linalg.norm(delta_t, ord=2)\n",
    "    thetas[:, t+1:t+2] = theta_t + ALPHA*delta_t\n",
    "    ### END SOLUTION\n",
    "    \n",
    "theta_ga = thetas[:, MAX_STEP:]\n",
    "print(\"Your (hand) solution:\", my_theta.T.flatten())\n",
    "print(\"Computed solution:\", theta_ga.T.flatten())\n",
    "\n",
    "print(\"\\n=== Comparisons ===\")\n",
    "display(Math (r'\\dfrac{\\theta_0}{\\theta_2}:'))\n",
    "print(\"Your manual (hand-picked) solution is\", my_theta[0]/my_theta[2], \\\n",
    "      \", vs. MLE (via gradient ascent), which is\", theta_ga[0]/theta_ga[2])\n",
    "display(Math (r'\\dfrac{\\theta_1}{\\theta_2}:'))\n",
    "print(\"Your manual (hand-picked) solution is\", my_theta[1]/my_theta[2], \\\n",
    "      \", vs. MLE (via gradient ascent), which is\", theta_ga[1]/theta_ga[2])\n",
    "\n",
    "print(\"\\n=== The MLE solution, visualized ===\")\n",
    "ga_labels = gen_lin_discr_labels(points, theta_ga)\n",
    "df_ga = df.copy()\n",
    "df_ga['label'] = mark_matches(ga_labels, labels).astype (dtype=int)\n",
    "plot_lin_discr(theta_ga, df_ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "logreg_mle__check",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print (\"\\n=== Mismatch counts ===\")\n",
    "\n",
    "my_labels = gen_lin_discr_labels (points, my_theta)\n",
    "my_mismatches = len (labels) - count_matches (labels, my_labels)\n",
    "print (\"Your manual (hand-picked) solution has\", num_mismatches, \"mismatches.\")\n",
    "\n",
    "ga_labels = gen_lin_discr_labels (points, theta_ga)\n",
    "ga_mismatches = len (labels) - count_matches (labels, ga_labels)\n",
    "print (\"The MLE method produces\", ga_mismatches, \"mismatches.\")\n",
    "\n",
    "assert ga_mismatches <= 8\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-285313553daac698",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**The gradient ascent trajectory.** Let's take a look at how gradient ascent progresses. (You might try changing the $\\alpha$ parameter and see how it affects the results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d8f9722865e273c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n_ll_grid = 100\n",
    "x1 = np.linspace(-8., 0., n_ll_grid)\n",
    "\n",
    "x2 = np.linspace(-8., 0., n_ll_grid)\n",
    "x1_grid, x2_grid = np.meshgrid(x1, x2)\n",
    "\n",
    "ll_grid = np.zeros((n_ll_grid, n_ll_grid))\n",
    "for i1 in range(n_ll_grid):\n",
    "    for i2 in range(n_ll_grid):\n",
    "        theta_i1_i2 = np.array([[thetas[0, MAX_STEP]],\n",
    "                                [x1_grid[i1][i2]],\n",
    "                                [x2_grid[i1][i2]]])\n",
    "        ll_grid[i1][i2] = log_likelihood(theta_i1_i2, y, X)\n",
    "\n",
    "# Determine a color scale\n",
    "def v(x):\n",
    "    return -np.log(np.abs(x))\n",
    "    return x\n",
    "\n",
    "def v_inv(v):\n",
    "    return -np.exp(np.abs(v))\n",
    "    return v\n",
    "\n",
    "v_min, v_max = v(ll_grid.min()), v(ll_grid.max())\n",
    "v_range = v_max - v_min\n",
    "v_breaks = v_inv(np.linspace(v_min, v_max, 20))\n",
    "\n",
    "p = plt.contourf(x1, x2, ll_grid, v_breaks, cmap=plt.cm.get_cmap(\"winter\"))\n",
    "plt.xlabel('theta_0')\n",
    "plt.ylabel('theta_1')\n",
    "plt.title('log-likelihood')\n",
    "plt.colorbar()\n",
    "plt.plot(thetas[1, :], thetas[2, :], 'k*-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d6e06b8eee6066a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 5 (optional): Numerical optimization via Newton's method\n",
    "\n",
    "The fudge factor, $\\alpha$, in gradient ascent should give you pause. Can you choose the step size or direction in a better or more principled way?\n",
    "\n",
    "One idea is [_Newton's method_](http://www.math.uiuc.edu/documenta/vol-ismp/13_deuflhard-peter.pdf), summarized below.\n",
    "\n",
    "> This part of the notebook has additional exercises, but they are all worth 0 points. (So if you submit something that is incomplete or fails the test cells, you won't lose any points.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1291a382cc089ccb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**The basic idea, in 1-D.** Suppose you start at a point $x$ and, assuming you are not yet at the optimum, you have decided to take a step of size $s$. That puts you at $f(x + s)$.\n",
    "\n",
    "How do you choose $s$? In gradient ascent, you do so by following the gradient, which points in an \"upward\" direction.\n",
    "\n",
    "In Newton's method, you will pick $s$ in a different way: choose $s$ to maximize $f(x + s)$.\n",
    "\n",
    "That should strike you as circular; the whole problem from the beginning was to maximize $f(x)$. The trick, in this case, is not to maximize $f(x+s)$ directly; rather, let's replace it with some approximation, $q(s) \\approx f(x+s)$, and maximize $q(s)$ instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e02155dc7b9e8d8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "A simple choice for $q(s)$ is a _quadratic_ function in $s$. This choice is motivated by two factors: (a) since it's quadratic, it should have some sort of extreme point (and hopefully an actual maximum), and (b) it is a higher-order approximation than a linear one, and so hopefully more accurate than a linear one as well.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  f(x + s)\n",
    "    & \\approx & f(x) + s \\dfrac{df}{dx} + \\frac{1}{2} s^2 \\dfrac{d^2 f}{dx^2}\n",
    "    & \\equiv  & q(s).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5905db1304c73510",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "To maximize $q(s)$, take its derivative and then solve for the $s_*$ such that $q(s_*) = 0$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\left.\\dfrac{dq}{ds}\\right|_{s=s_*}\n",
    "    & = & \\dfrac{df}{dx} + s_* \\dfrac{d^2 f}{dx^2} = 0 \\\\\n",
    "  \\implies s_*\n",
    "    & = & -\\dfrac{df}{dx} \\left(\\dfrac{d^2 f}{dx^2}\\right)^{-1}.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is, the optimal step $s_*$ is the negative of the first derivative of $f$ divided by its second derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-faaaee9d546691b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Generalizing to higher dimensions.** To see how this procedure works in higher dimensions, you will need not only the gradient of $f(x)$, but also its _Hessian_, which is the moral equivalent of a second derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-347083bda5bbf939",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "_Definition:_ **the Hessian.** Let $f(v)$ be a function that takes a _vector_ $v$ of length $n$ as input and returns a scalar. The _Hessian_ of $f(v)$ is an $n \\times n$ matrix, $H_v(f)$, whose entries are all $n^2$ possible second-order partial derivatives with respect to the components of $v$. That is, let $h_{ij}$ be the $(i, j)$ element of $H_v(f)$. Then we define\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  h_{ij}\n",
    "    & \\equiv & \\dfrac{\\partial^2}{\\partial v_i \\partial v_j} f(v).\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5a14e707e62ddeb0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Armed with a Hessian, the Newton step is defined as follows, by direct analogy to the 1-D case. First, the Taylor series approximation of $f(x + s)$ for multidimensional variables is, as it happens,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  f(x + s)\n",
    "    & \\approx & f(x) + {s^T \\, \\nabla_x \\, f} + {\\frac{1}{2}\\,s^T H_x(f) \\, s}\n",
    "    & \\equiv  & q(s).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ccc44690691a89f3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As in the 1-D case, we want to find an extreme point of $q(s)$. Taking its \"derivative\" (gradient), $\\nabla_s q$, and setting it to 0 yields,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\nabla_s \\, q(s)\n",
    "    & = & \\nabla_x \\, f(x) + H_x(f) \\, s = 0 \\\\\n",
    "  \\implies\n",
    "  H_x(f) \\cdot s\n",
    "    & = & -\\, \\nabla_x \\, f(x).\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In other words, to choose the next step $s$, Newton's method suggests that you must _solve_ a system of linear equations, where the matrix is the Hessian of $f$ and the right-hand side is the negative gradient of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bcd0d7580c2fd26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Summary: Newton's method.** Summarizing the main ideas from above, Newton's method to maximize the scalar objective function $f(x)$ where $x$ is a vector, consists of the following steps:\n",
    "\n",
    "* Start with some initial guess $x(0)$.\n",
    "* At step $t$, compute the _search direction_ $s(t)$ by solving $H_{x(t)}(f) \\cdot s(t) = -\\, \\nabla_x \\, f(x(t))$.\n",
    "* Compute a new (and hopefully improved) guess by the update, $x(t+1) \\leftarrow x(t) + s(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f2fc8779174f662",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Implementing logistic regression via a Newton-based MLE\n",
    "\n",
    "To perform MLE for the logistic regression model using Newton's method, you need both the gradient of the log-likelihood as well as the Hessian. You already know how to compute the gradient from the preceding exercises; so what about the Hessian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1a0bfe46ecd0f90c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Notationally, that calculation will be a little bit easier to write down and program with the following definition.\n",
    "\n",
    "_Definition:_ **Elementwise product**. Let $A \\equiv (a_{ij})$ and $B \\equiv (b_{ij})$ be $m \\times n$ matrices. Denote the _elementwise product_ of $A$ and $B$ by $A \\odot B$. That is, if $C = A \\odot B$, then element $c_{ij} = a_{ij} \\cdot b_{ij}$.\n",
    "\n",
    "If $A$ is $m \\times n$ but $B$ is instead just $m \\times 1$, then we will \"auto-extend\" $B$. Put differently, if $B$ has the same number of rows as $A$ but only 1 column, then we will take $C = A \\odot B$ to have elements $c_{ij} = a_{ij} \\cdot b_{i}$.\n",
    "\n",
    "In Python, you can use [`np.multiply()`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.multiply.html) for elementwise multiplication of Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-595e98fd54c852a2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "B = np.array([[-1, 2, -3],\n",
    "              [4, -5, 6]])\n",
    "\n",
    "print(np.multiply(A, B)) # elementwise product\n",
    "print()\n",
    "print(np.multiply(A, B[:, 0:1])) # \"auto-extend\" version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-986f6b8158495c0a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 10 (optional; ungraded)**. Show that the Hessian of the log-likelihood for logistic regression is\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  H_{\\theta} \\left( \\mathcal{L}(\\theta; l, X) \\right)\n",
    "    & = & -\\left( X \\odot G(X \\theta) \\right)^T \\left( X \\odot G(-X \\theta) \\right).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c2727799bbf1548",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 11** (0 points). Implement a function to compute the Hessian of the log-likelihood. The signature of your function should be,\n",
    "\n",
    "```python\n",
    "  def hess_log_likelihood (theta, y, X):\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "hessian_log_likelihood",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def hess_log_likelihood(theta, y, X):\n",
    "    \"\"\"Returns the Hessian of the log-likelihood.\"\"\"\n",
    "    z = X.dot(theta)\n",
    "    A = np.multiply(X, logistic(z))\n",
    "    B = np.multiply(X, logistic(-z))\n",
    "    return -A.T.dot(B)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "hess_log_likelihood__check",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `hess_log_likelihood__check`\n",
    "\n",
    "if False:\n",
    "    d_hess_soln = 20\n",
    "    m_hess_soln = 501\n",
    "    theta_hess_soln = np.random.random ((d_hess_soln+1, 1)) * 2.0 - 1.0\n",
    "    y_hess_soln = np.random.randint (low=0, high=2, size=(m_hess_soln, 1))\n",
    "    X_hess_soln = np.random.random ((m_hess_soln, d_hess_soln+1)) * 2.0 - 1.0\n",
    "    X_hess_soln[:, 0] = 1.0\n",
    "    L_hess_soln = hess_log_likelihood (theta_hess_soln, y_hess_soln, X_hess_soln)\n",
    "    np.savez_compressed ('{}hess_log_likelihood_soln'.format(DATA_PATH),\n",
    "                         d_hess_soln, m_hess_soln, theta_hess_soln, y_hess_soln, X_hess_soln, L_hess_soln)\n",
    "\n",
    "npzfile_hess_soln = np.load ('{}hess_log_likelihood_soln.npz'.format(DATA_PATH))\n",
    "d_hess_soln = npzfile_hess_soln['arr_0']\n",
    "m_hess_soln = npzfile_hess_soln['arr_1']\n",
    "theta_hess_soln = npzfile_hess_soln['arr_2']\n",
    "y_hess_soln = npzfile_hess_soln['arr_3']\n",
    "X_hess_soln = npzfile_hess_soln['arr_4']\n",
    "L_hess_soln = npzfile_hess_soln['arr_5']\n",
    "\n",
    "L_hess_you = hess_log_likelihood(theta_hess_soln, y_hess_soln, X_hess_soln)\n",
    "your_hess_err = np.max(np.abs(L_hess_you/L_hess_soln - 1.0))\n",
    "display(Math(r'\\left\\|\\dfrac{H_{\\tiny \\mbox{yours}} - H_{\\tiny \\mbox{solution}}}{H_{\\tiny \\mbox{solution}}}\\right\\|_\\infty \\approx %g' % your_hess_err))\n",
    "assert your_hess_err <= 1e-12\n",
    "\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d26e099ed14922a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 12** (0 points). Finish the implementation of a Newton-based MLE procedure for the logistic regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "logreg_mle_newton",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "MAX_STEP = 10\n",
    "\n",
    "# Get the data coordinate matrix, X, and labels vector, l\n",
    "X = points\n",
    "y = labels.astype(dtype=float)\n",
    "\n",
    "# Store *all* guesses, for subsequent analysis\n",
    "thetas_newt = np.zeros((3, MAX_STEP+1))\n",
    "\n",
    "for t in range(MAX_STEP):\n",
    "    ### BEGIN SOLUTION\n",
    "    theta_t = thetas_newt[:, t:t+1]\n",
    "    g_t = grad_log_likelihood(theta_t, y, X)\n",
    "    H_t = hess_log_likelihood(theta_t, y, X)\n",
    "    s_t = np.linalg.solve(H_t, -g_t)\n",
    "    thetas_newt[:, t+1:t+2] = theta_t + s_t\n",
    "    ### END SOLUTION\n",
    "\n",
    "theta_newt = thetas_newt[:, MAX_STEP:]\n",
    "print (\"Your (hand) solution:\", my_theta.T.flatten())\n",
    "print (\"Computed solution:\", theta_newt.T.flatten())\n",
    "\n",
    "print (\"\\n=== Comparisons ===\")\n",
    "display (Math (r'\\dfrac{\\theta_0}{\\theta_2}:'))\n",
    "print (\"Your manual (hand-picked) solution is\", my_theta[0]/my_theta[2], \\\n",
    "      \", vs. MLE (via Newton's method), which is\", theta_newt[0]/theta_newt[2])\n",
    "display (Math (r'\\dfrac{\\theta_1}{\\theta_2}:'))\n",
    "print (\"Your manual (hand-picked) solution is\", my_theta[1]/my_theta[2], \\\n",
    "      \", vs. MLE (via Newton's method), which is\", theta_newt[1]/theta_newt[2])\n",
    "\n",
    "print (\"\\n=== The MLE solution, visualized ===\")\n",
    "newt_labels = gen_lin_discr_labels(points, theta_newt)\n",
    "df_newt = df.copy()\n",
    "df_newt['label'] = mark_matches(newt_labels, labels).astype (dtype=int)\n",
    "plot_lin_discr(theta_newt, df_newt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "logreg_mle_newt__check",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `logreg_mle_newt__check`\n",
    "\n",
    "print (\"\\n=== Mismatch counts ===\")\n",
    "\n",
    "my_labels = gen_lin_discr_labels (points, my_theta)\n",
    "my_mismatches = len (labels) - count_matches (labels, my_labels)\n",
    "print (\"Your manual (hand-picked) solution has\", num_mismatches, \"mismatches.\")\n",
    "\n",
    "newt_labels = gen_lin_discr_labels (points, theta_newt)\n",
    "newt_mismatches = len (labels) - count_matches (labels, newt_labels)\n",
    "print (\"The MLE+Newton method produces\", newt_mismatches, \"mismatches.\")\n",
    "\n",
    "assert newt_mismatches <= ga_mismatches\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-927d3fb6e847780f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell creates a contour plot of the log-likelihood, as done previously in this notebook. Add code to display the trajectory taken by Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc5d42b4438b3def",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "p = plt.contourf(x1, x2, ll_grid, cmap=plt.cm.get_cmap(\"winter\"))\n",
    "plt.xlabel('theta_0')\n",
    "plt.ylabel('theta_1')\n",
    "plt.title('Trajectory taken by Newton\\'s method')\n",
    "plt.colorbar()\n",
    "plt.plot(thetas_newt[1, :], thetas_newt[2, :], 'k*-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4ac2618cab7f5ab5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "How many steps does this optimization procedure take compared to gradient ascent? What is the tradeoff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c17d1735dac419ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Fin!** This notebook ends here. Don't forget to submit it!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
