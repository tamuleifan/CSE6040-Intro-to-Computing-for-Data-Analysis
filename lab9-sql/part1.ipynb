{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ec8cf1650bc52f8313832f3d3611786b",
     "grade": false,
     "grade_id": "cell-ca9366ee8c91d9c6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Important note!** Before you turn in this lab notebook, make sure everything runs as expected:\n",
    "\n",
    "- First, **restart the kernel** -- in the menubar, select Kernel$\\rightarrow$Restart.\n",
    "- Then **run all cells** -- in the menubar, select Cell$\\rightarrow$Run All.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "ac05e2a5-19d3-459a-be72-15ac33423f4f"
    }
   },
   "source": [
    "# Lesson 2: NYC 311 calls\n",
    "\n",
    "This notebook derives from a [demo by the makers of plot.ly](https://plot.ly/ipython-notebooks/big-data-analytics-with-pandas-and-sqlite/). We've adapted it to use [Bokeh (and HoloViews)](http://bokeh.pydata.org/en/latest/).\n",
    "\n",
    "You will start with a large database of complaints filed by residents of New York City since 2010 via 311 calls. The full dataset is available at the [NYC open data portal](https://nycopendata.socrata.com/data). At about 6 GB and 10 million complaints, you can infer that a) you might not want to read it all into memory at once, and b) NYC residents have a lot to complain about. (Maybe only conclusion \"a\" is valid.) The notebook then combines the use of `sqlite`, `pandas`, and `bokeh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module setup\n",
    "\n",
    "Before diving in, run the following cells to preload some functions you'll need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "19cc7eb8-4336-46cc-b3e3-fe64f194fc73"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "e5550663-09e1-42e4-a183-f7a2289dd76c"
    }
   },
   "source": [
    "We'll also need some functionality from earlier notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "48fbd005-a500-48f3-ad9f-66d4658824af"
    }
   },
   "outputs": [],
   "source": [
    "def canonicalize_tibble(X):\n",
    "    var_names = sorted(X.columns)\n",
    "    Y = X[var_names].copy()\n",
    "    Y.sort_values(by=var_names, inplace=True)\n",
    "    Y.reset_index(drop=True, inplace=True)\n",
    "    return Y\n",
    "\n",
    "def tibbles_are_equivalent (A, B):\n",
    "    A_canonical = canonicalize_tibble(A)\n",
    "    B_canonical = canonicalize_tibble(B)\n",
    "    cmp = A_canonical.eq(B_canonical)\n",
    "    return cmp.all().all()\n",
    "\n",
    "def cast(df, key, value, join_how='outer'):\n",
    "    \"\"\"Casts the input data frame into a tibble,\n",
    "    given the key column and value column.\n",
    "    \"\"\"\n",
    "    assert type(df) is pd.DataFrame\n",
    "    assert key in df.columns and value in df.columns\n",
    "    assert join_how in ['outer', 'inner']\n",
    "    \n",
    "    fixed_vars = df.columns.difference([key, value])\n",
    "    tibble = pd.DataFrame(columns=fixed_vars) # empty frame    \n",
    "    new_vars = df[key].unique()\n",
    "    for v in new_vars:\n",
    "        df_v = df[df[key] == v]\n",
    "        del df_v[key]\n",
    "        df_v = df_v.rename(columns={value: v})\n",
    "        tibble = tibble.merge(df_v,\n",
    "                              on=list(fixed_vars),\n",
    "                              how=join_how)    \n",
    "    return tibble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, some of the test cells will need some auxiliary files, which the following code cell will check for and, if they are missing, download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import hashlib\n",
    "import io\n",
    "\n",
    "def download(file, url_suffix=None, checksum=None):\n",
    "    if url_suffix is None:\n",
    "        url_suffix = file\n",
    "        \n",
    "    if os.path.exists('.voc'):\n",
    "        url = 'https://cse6040.gatech.edu/datasets/{}'.format(url_suffix)\n",
    "    else:\n",
    "        url = 'https://github.com/cse6040/labs-fa17/raw/master/{}'.format(url_suffix)\n",
    "    if os.path.exists(file):\n",
    "        print(\"[{}]\\n==> '{}' is already available.\".format(url, file))\n",
    "    else:\n",
    "        print(\"[{}] Downloading...\".format(url))\n",
    "        r = requests.get(url)\n",
    "        with open(file, 'w', encoding=r.encoding) as f:\n",
    "            f.write(r.text)\n",
    "            \n",
    "    if checksum is not None:\n",
    "        with io.open(file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            body = f.read()\n",
    "            body_checksum = hashlib.md5(body.encode('utf-8')).hexdigest()\n",
    "            assert body_checksum == checksum, \\\n",
    "                \"Downloaded file '{}' has incorrect checksum: '{}' instead of '{}'\".format(file, body_checksum, checksum)\n",
    "            print(\"==> Checksum test passes: {}\".format(checksum))\n",
    "    \n",
    "    print(\"==> '{}' is ready!\\n\".format(file))\n",
    "    \n",
    "auxfiles = {'df_complaints_by_city_soln.csv': '2a82e5856d5a267db9aafc26f16c3ae1',\n",
    "            'df_complaints_by_hour_soln.csv': 'f06fcd917876d51ad52ddc13b2fee69e',\n",
    "            'df_noisy_by_hour_soln.csv': '30f3fa7c753d4d3f4b3edfa1f6d05bcc',\n",
    "            'df_plot_stacked_fraction_soln.csv': '2ca04a3eb24ccc37ddd0f8f5917fb27a'}\n",
    "\n",
    "for filename, checksum in auxfiles.items():\n",
    "    download(filename, url_suffix='{}/{}'.format('lab9-sql', filename), checksum=checksum)\n",
    "    \n",
    "print(\"(Auxiliary files appear to be ready.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "e1cd3bf9-e15d-4d7a-98d0-33ba5f0d3bc8"
    }
   },
   "source": [
    "## Viz setup\n",
    "\n",
    "This notebook includes some simple visualizations. This section just ensures you have the right software setup to follow along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "44a7ba22-db47-4da9-bf5c-1fae5abe3119"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build a Pandas data frame\n",
    "names = ['Bob','Jessica','Mary','John','Mel']\n",
    "births = [968, 155, 77, 578, 973]\n",
    "name_birth_pairs = list(zip(names, births))\n",
    "baby_names = pd.DataFrame(data=name_birth_pairs, columns=['Names', 'Births'])\n",
    "display(baby_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "52f96d89-7523-4c1f-aaf2-8a3b7cd567fe"
    }
   },
   "outputs": [],
   "source": [
    "import holoviews as hv # Replacement for bokeh.charts / bkcharts\n",
    "hv.extension('bokeh')\n",
    "from holoviews import Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "11732f28-b968-4f34-95fe-df6198342524"
    }
   },
   "outputs": [],
   "source": [
    "%%opts Bars [width=640 height=320]\n",
    "Bars(baby_names, kdims=['Names'], vdims=['Births'], color='Names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the HoloViews interface (above), some of the visualizations will use the Bokeh mid-level interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from: https://bokeh.pydata.org/en/latest/docs/user_guide/categorical.html#userguide-categorical\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.core.properties import value\n",
    "\n",
    "def make_stacked_bar(df, x_var, bar_vars, kwargs_figure={}):\n",
    "    assert type(x_var) is str, \"x-variable should be a string but isn't.\"\n",
    "    assert all([b in df.columns for b in bar_vars]), \"Data frame is missing one or more columns: {}\".format(bar_vars)\n",
    "    \n",
    "    from bokeh.palettes import brewer\n",
    "    assert len(bar_vars) in brewer['Dark2'], \"Not enough colors.\"\n",
    "    \n",
    "    x = list(df[x_var])\n",
    "    colors = brewer['Dark2'][len(bar_vars)]\n",
    "    legend = [value(b) for b in bar_vars]\n",
    "    source = ColumnDataSource(data=df)\n",
    "\n",
    "    p = figure(x_range=x, **kwargs_figure)\n",
    "    p.vbar_stack(bar_vars, x=x_var, width=0.9,\n",
    "             fill_color=colors, line_color=None,\n",
    "             legend=legend,\n",
    "             source=source)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup\n",
    "\n",
    "You'll also need the NYC 311 calls dataset. What we've provided is actually a small subset (about 250+ MiB) of the full data as of 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import hashlib\n",
    "import io\n",
    "\n",
    "def on_vocareum():\n",
    "    return os.path.exists('.voc')\n",
    "\n",
    "if on_vocareum():\n",
    "    DB_FILENAME = None # TBD\n",
    "else:\n",
    "    DB_FILENAME = 'NYC-311-2M.db'\n",
    "    \n",
    "if not os.path.exists(DB_FILENAME):\n",
    "    url = 'https://onedrive.live.com/download?cid=FD520DDC6BE92730&resid=FD520DDC6BE92730%21616&authkey=AEeP_4E1uh-vyDE'\n",
    "    print(\"Downloading: {} ...\".format(url))\n",
    "    r = requests.get(url)\n",
    "    with open(file, 'w', encoding=r.encoding) as f:\n",
    "        f.write(r.text)\n",
    "\n",
    "DB_CHECKSUM = 'f48eba2fb06e8ece7479461ea8c6dee9'\n",
    "with io.open(DB_FILENAME, 'rb') as f:\n",
    "    body = f.read()\n",
    "    body_checksum = hashlib.md5(body).hexdigest()\n",
    "    assert body_checksum == DB_CHECKSUM, \\\n",
    "        \"Database file '{}' has an incorrect checksum: '{}' instead of '{}'\".format(DB_FILENAME,\n",
    "                                                                                    body_checksum,\n",
    "                                                                                    DB_CHECKSUM)\n",
    "    \n",
    "print(\"'{}' is ready!\".format(DB_FILENAME))  \n",
    "print(\"\\n(All data appears to be ready.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connecting.** Let's open up a connection to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "e2134172-76ad-4ffb-86c9-161df8a53355"
    }
   },
   "outputs": [],
   "source": [
    "# Connect\n",
    "import sqlite3 as db\n",
    "disk_engine = db.connect(DB_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "31873b5c-d2fc-4d92-853a-9a4bb84b0510"
    }
   },
   "source": [
    "**Preview the data.** This sample database has just a single table, named `data`. Let's query it and see how long it takes to read. To carry out the query, we will use the SQL reader built into `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "f6dbf63b-4745-4f56-8d73-d4ce71e68f7c"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print (\"Reading ...\")\n",
    "start_time = time.time ()\n",
    "\n",
    "# Perform SQL query through the disk_engine connection.\n",
    "# The return value is a pandas data frame.\n",
    "df = pd.read_sql_query ('select * from data', disk_engine)\n",
    "\n",
    "elapsed_time = time.time () - start_time\n",
    "print (\"==> Took %g seconds.\" % elapsed_time)\n",
    "\n",
    "# Dump the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "90bcd6d3-b28a-4d65-8967-1fe09939c407"
    }
   },
   "source": [
    "**Partial queries: `LIMIT` clause.** The preceding command was overkill for what we wanted, which was just to preview the table. Instead, we could have used the `LIMIT` option to ask for just a few results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "d2e06725-6f72-4b90-858c-80861cd77086"
    }
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  select *\n",
    "    from data\n",
    "    limit 5\n",
    "'''\n",
    "start_time = time.time ()\n",
    "df = pd.read_sql_query (query, disk_engine)\n",
    "elapsed_time = time.time () - start_time\n",
    "print (\"==> LIMIT version took %g seconds.\" % elapsed_time)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "202da090-d21b-4e0c-ab74-e82da32305a6"
    }
   },
   "source": [
    "**Grouping Information: GROUP BY operator.** The GROUP BY operator lets you group information using a particular column or multiple columns of the table. The output generated is more of a pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "07d42c35-bfc1-4083-b851-af26039ab7d0"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  select ComplaintType, Descriptor, Agency\n",
    "    from data\n",
    "    GROUP BY ComplaintType\n",
    "    limit 10\n",
    "'''\n",
    "\n",
    "df = pd.read_sql_query(query, disk_engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "df7cfcaa-10e4-4374-8f39-3d2ead1b4e34"
    }
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  select ComplaintType, Descriptor, Agency\n",
    "    from data\n",
    "    limit 10\n",
    "'''\n",
    "\n",
    "df = pd.read_sql_query(query, disk_engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "de9b4364-16ef-4c59-8d9a-1c6749c93f19"
    }
   },
   "source": [
    "**Set membership: `IN` operator.** Another common idiom is to ask for rows whose attributes fall within a set, for which you can use the `IN` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "0d0041f8-097f-4a64-a2c0-ad0a4ea6a71a"
    }
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  select ComplaintType, Descriptor, Agency\n",
    "    from data\n",
    "    where Agency IN (\"NYPD\", \"DOB\")\n",
    "    limit 10\n",
    "'''\n",
    "\n",
    "df = pd.read_sql_query (query, disk_engine)\n",
    "df.head ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "35d500f1-b17b-48cf-ab12-1267d8aeb485"
    }
   },
   "source": [
    "**Finding unique values: `DISTINCT` qualifier.** Yet another common idiom is to ask for the unique values of some attribute, for which you can use the `DISTINCT` qualifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "a9206a4f-7d5e-4055-9221-cbd3c52fd6e2"
    }
   },
   "outputs": [],
   "source": [
    "query = 'select DISTINCT City FROM data'\n",
    "df = pd.read_sql_query(query, disk_engine)\n",
    "\n",
    "print(\"Found {} unique cities. The first few are:\".format(len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "b1c2afab-abbf-47c7-9938-7aa36d3382d7"
    }
   },
   "source": [
    "**Renaming columns: `AS` operator.** Sometimes you might want to rename a result column. For instance, the following query counts the number of complaints by \"Agency,\" using the `COUNT(*)` function and `GROUP BY` clause, which we discussed in an earlier lab. If you wish to refer to the counts column of the resulting data frame, you can give it a more \"friendly\" name using the `AS` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "9b495c9d-28a8-4074-a2ff-43e5481a9d5f"
    }
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  select Agency, count(*) as NumComplaints\n",
    "    from data\n",
    "    group by Agency\n",
    "'''\n",
    "df = pd.read_sql_query(query, disk_engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "b27b5f58-ffc9-4e1c-bf5b-76149575dfa5"
    }
   },
   "source": [
    "**Ordering results: `ORDER` clause.** You can also order the results. For instance, suppose we want to execute the previous query by number of complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "2f09ad6d-3920-465c-ac44-6438fc3f3ee5"
    }
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  select Agency, count(*) as NumComplaints\n",
    "    from data\n",
    "    group by Agency\n",
    "    order by NumComplaints\n",
    "'''\n",
    "df = pd.read_sql_query(query, disk_engine)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "1cb6f520-9cda-49ef-bf62-1863dad7414a"
    }
   },
   "source": [
    "Note that the above example prints the bottom (tail) of the data frame. You could have also asked for the query results in reverse (descending) order, by prefixing the `ORDER BY` attribute with a `-` (minus) symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "304e200a-2d98-482c-8f29-c011ec2c5482"
    }
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  select Agency, count(*) as NumComplaints\n",
    "    from data\n",
    "    group by Agency\n",
    "    order by -NumComplaints\n",
    "'''\n",
    "df = pd.read_sql_query(query, disk_engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "aeaca6a3-b097-4844-8174-07fca9d185cc"
    }
   },
   "source": [
    "And of course we can plot all of this data!\n",
    "\n",
    "**Exercise 0** (ungraded). Run the following code cell, which will create an interactive bar chart from the data in the previous query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4783aa747bd23e496521c5f473e98335",
     "grade": true,
     "grade_id": "exercise_0",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%opts Bars [width=640 height=320]\n",
    "chart = Bars(df[:20], kdims=['Agency'], vdims=[('NumComplaints', '# complaints')],\n",
    "             label='Top 20 agencies by number of complaints')\n",
    "chart(plot={'xrotation': 25})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "c2798673-ea3c-4699-a6e3-d260ea898fe7"
    }
   },
   "source": [
    "**Exercise 1** (2 points). Create a string, `query`, containing an SQL query that will return the number of complaints by type. The columns should be named `type` and `freq`, and the results should be sorted in descending order by `freq`.\n",
    "\n",
    "> What is the most common type of complaint? What, if anything, does it tell you about NYC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "bc4b8e86d61cd420c80b8a0ffbfd8dc8",
     "grade": false,
     "grade_id": "complaints",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "nbpresent": {
     "id": "e295216c-a5d8-498c-884d-a971991f23a0"
    }
   },
   "outputs": [],
   "source": [
    "del query\n",
    "\n",
    "# Define a variable named `query` containing your solution\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Runs your `query`:\n",
    "df_complaint_freq = pd.read_sql_query(query, disk_engine)\n",
    "df_complaint_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6bc805ab0c6387a9ea9869b00ee8f6a6",
     "grade": true,
     "grade_id": "complaints_test",
     "locked": true,
     "points": 2.0,
     "schema_version": 1,
     "solution": false
    },
    "nbpresent": {
     "id": "30c8f31e-b2f3-46fb-b07c-38af40417e8e"
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `complaints_test`\n",
    "\n",
    "print(\"Top 10 complaints:\")\n",
    "display(df_complaint_freq.head(10))\n",
    "\n",
    "assert set(df_complaint_freq.columns) == {'type', 'freq'}, \"Output columns should be named 'type' and 'freq', not {}\".format(set(df_complaint_freq.columns))\n",
    "\n",
    "soln = ['HEAT/HOT WATER', 'Street Condition', 'Street Light Condition', 'Blocked Driveway', 'Illegal Parking', 'UNSANITARY CONDITION', 'PAINT/PLASTER', 'Water System', 'PLUMBING', 'Noise', 'Noise - Street/Sidewalk', 'Traffic Signal Condition', 'Noise - Commercial', 'DOOR/WINDOW', 'WATER LEAK', 'Dirty Conditions', 'Sewer', 'Sanitation Condition', 'DOF Literature Request', 'ELECTRIC', 'Rodent', 'FLOORING/STAIRS', 'General Construction/Plumbing', 'Building/Use', 'Broken Muni Meter', 'GENERAL', 'Missed Collection (All Materials)', 'Benefit Card Replacement', 'Derelict Vehicle', 'Noise - Vehicle', 'Damaged Tree', 'Consumer Complaint', 'Derelict Vehicles', 'Taxi Complaint', 'Overgrown Tree/Branches', 'Graffiti', 'Snow', 'Opinion for the Mayor', 'APPLIANCE', 'Maintenance or Facility', 'Animal Abuse', 'Dead Tree', 'HPD Literature Request', 'Root/Sewer/Sidewalk Condition', 'SAFETY', 'Elevator', 'Food Establishment', 'SCRIE', 'Air Quality', 'Agency Issues', 'Construction', 'Highway Condition', 'Other Enforcement', 'Water Conservation', 'Sidewalk Condition', 'Indoor Air Quality', 'Street Sign - Damaged', 'Traffic', 'Plumbing', 'Fire Safety Director - F58', 'Homeless Person Assistance', 'Homeless Encampment', 'Special Enforcement', 'Street Sign - Missing', 'Noise - Park', 'Vending', 'For Hire Vehicle Complaint', 'Food Poisoning', 'Special Projects Inspection Team (SPIT)', 'Hazardous Materials', 'Electrical', 'DOT Literature Request', 'Litter Basket / Request', 'Taxi Report', 'Illegal Tree Damage', 'DOF Property - Reduction Issue', 'Unsanitary Animal Pvt Property', 'Asbestos', 'Lead', 'Vacant Lot', 'DCA / DOH New License Application Request', 'Street Sign - Dangling', 'Smoking', 'Violation of Park Rules', 'OUTSIDE BUILDING', 'Animal in a Park', 'Noise - Helicopter', 'School Maintenance', 'DPR Internal', 'Boilers', 'Industrial Waste', 'Sweeping/Missed', 'Overflowing Litter Baskets', 'Non-Residential Heat', 'Curb Condition', 'Drinking', 'Standing Water', 'Indoor Sewage', 'Water Quality', 'EAP Inspection - F59', 'Derelict Bicycle', 'Noise - House of Worship', 'DCA Literature Request', 'Recycling Enforcement', 'ELEVATOR', 'DOF Parking - Tax Exemption', 'Broken Parking Meter', 'Request for Information', 'Taxi Compliment', 'Unleashed Dog', 'Urinating in Public', 'Unsanitary Pigeon Condition', 'Investigations and Discipline (IAD)', 'Bridge Condition', 'Ferry Inquiry', 'Bike/Roller/Skate Chronic', 'Public Payphone Complaint', 'Vector', 'BEST/Site Safety', 'Sweeping/Inadequate', 'Disorderly Youth', 'Found Property', 'Mold', 'Senior Center Complaint', 'Fire Alarm - Reinspection', 'For Hire Vehicle Report', 'City Vehicle Placard Complaint', 'Cranes and Derricks', 'Ferry Complaint', 'Illegal Animal Kept as Pet', 'Posting Advertisement', 'Harboring Bees/Wasps', 'Panhandling', 'Scaffold Safety', 'OEM Literature Request', 'Plant', 'Bus Stop Shelter Placement', 'Collection Truck Noise', 'Beach/Pool/Sauna Complaint', 'Complaint', 'Compliment', 'Illegal Fireworks', 'Fire Alarm - Modification', 'DEP Literature Request', 'Drinking Water', 'Fire Alarm - New System', 'Poison Ivy', 'Bike Rack Condition', 'Emergency Response Team (ERT)', 'Municipal Parking Facility', 'Tattooing', 'Unsanitary Animal Facility', 'Animal Facility - No Permit', 'Miscellaneous Categories', 'Misc. Comments', 'Literature Request', 'Special Natural Area District (SNAD)', 'Highway Sign - Damaged', 'Public Toilet', 'Adopt-A-Basket', 'Ferry Permit', 'Invitation', 'Window Guard', 'Parking Card', 'Illegal Animal Sold', 'Stalled Sites', 'Open Flame Permit', 'Overflowing Recycling Baskets', 'Highway Sign - Missing', 'Public Assembly', 'DPR Literature Request', 'Fire Alarm - Addition', 'Lifeguard', 'Transportation Provider Complaint', 'DFTA Literature Request', 'Bottled Water', 'Highway Sign - Dangling', 'DHS Income Savings Requirement', 'Legal Services Provider Complaint', 'Foam Ban Enforcement', 'Tunnel Condition', 'Calorie Labeling', 'Fire Alarm - Replacement', 'X-Ray Machine/Equipment', 'Sprinkler - Mechanical', 'Hazmat Storage/Use', 'Tanning', 'Radioactive Material', 'Rangehood', 'SRDE', 'Squeegee', 'Building Condition', 'SG-98', 'Standpipe - Mechanical', 'AGENCY', 'Forensic Engineering', 'Public Assembly - Temporary', 'VACANT APARTMENT', 'Laboratory', 'SG-99']\n",
    "assert all(soln == df_complaint_freq['type'])\n",
    "\n",
    "print(\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "044baaf9-9926-4afb-9e03-85073ca711c3"
    }
   },
   "source": [
    "Let's also visualize the result, as a bar chart showing complaint types on the x-axis and the number of complaints on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "e61968e4-5052-4825-9c07-d5221f507ed2"
    }
   },
   "outputs": [],
   "source": [
    "%%opts Bars [width=800 height=320]\n",
    "chart = Bars(df_complaint_freq[:25], kdims=['type'], vdims=[('freq', '# complaints')],\n",
    "             label='Top 25 complaints by type')\n",
    "chart(plot={'xrotation': 25})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbpresent": {
     "id": "887dcef2-10b4-4cbb-a282-ad93dd89332c"
    }
   },
   "source": [
    "# Lesson 3: More SQL stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "13ad6402-b7cf-4426-8a8f-6758fa07aec0"
    }
   },
   "source": [
    "**Simple substring matching: the `LIKE` operator.** Suppose we just want to look at the counts for all complaints that have the word `noise` in them. You can use the `LIKE` operator combined with the string wildcard, `%`, to look for case-insensitive substring matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "cdc97977-5b18-4e09-aa88-49458e4f6921"
    }
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  select ComplaintType as type, count(*) as freq\n",
    "    from data\n",
    "    where ComplaintType like '%noise%'\n",
    "    group by type\n",
    "    order by -freq\n",
    "'''\n",
    "\n",
    "df_noisy = pd.read_sql_query(query, disk_engine)\n",
    "df_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "e86be66c-aec9-4135-8f3e-b7fabaa2e800"
    }
   },
   "source": [
    "**Exercise 2** (2 points). Create a string variable, `query`, that contains an SQL query that will return the top 10 cities with the largest number of complaints, in descending order. It should return a table with two columns, one named `name` holding the name of the city, and one named `freq` holding the number of complaints by that city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "65b1f5bfe46bac2ccf983ccf68c58a98",
     "grade": false,
     "grade_id": "whiny_cities",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "nbpresent": {
     "id": "c18e68d3-61ed-4258-8e1b-aac0e538bca4"
    }
   },
   "outputs": [],
   "source": [
    "del query\n",
    "\n",
    "# Define your `query`, here:\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Runs your `query`:\n",
    "df_whiny_cities = pd.read_sql_query(query, disk_engine)\n",
    "df_whiny_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f4750ca9601e5b318477ba2c2ed2a545",
     "grade": true,
     "grade_id": "whiny_cities__test",
     "locked": true,
     "points": 2.0,
     "schema_version": 1,
     "solution": false
    },
    "nbpresent": {
     "id": "2cb6a7f7-2e1c-4d88-aef6-ea93a256fc17"
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `whiny_cities__test`\n",
    "\n",
    "assert df_whiny_cities['name'][0] == 'BROOKLYN'\n",
    "assert df_whiny_cities['name'][1] == 'NEW YORK'\n",
    "assert df_whiny_cities['name'][2] == 'BRONX'\n",
    "assert df_whiny_cities['name'][3] is None\n",
    "assert df_whiny_cities['name'][4] == 'STATEN ISLAND'\n",
    "\n",
    "print (\"\\n(Passed partial test.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "d5440726-77c1-4467-b6a6-dcb584cdf831"
    }
   },
   "source": [
    "You should notice two odd bits: cities are treated in a _case-sensitive_ manner and `None` appears as a city. (Presumably this setting occurs when a complaint is non-localized or the city is not otherwise specified.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "dc4f178f-e10c-48e3-9b89-e3864b1bd2b9"
    }
   },
   "source": [
    "**Case-insensitive grouping: `COLLATE NOCASE`.** One way to carry out the preceding query in a case-insensitive way is to add a `COLLATE NOCASE` qualifier to the `GROUP BY` clause.\n",
    "\n",
    "Let's filter out the 'None' cases as well, while we are at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "224ca27b-cf76-47d5-a914-918d217aa2d4"
    }
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  SELECT City as name, COUNT(*) AS freq\n",
    "    FROM data\n",
    "    WHERE name <> 'None'\n",
    "    GROUP BY name COLLATE NOCASE\n",
    "    ORDER BY -freq\n",
    "    LIMIT 10\n",
    "'''\n",
    "df_whiny_cities2 = pd.read_sql_query(query, disk_engine)\n",
    "df_whiny_cities2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "806365e8-6902-48d4-b0b8-eef52f8149eb"
    }
   },
   "source": [
    "Brooklynites are complainers, evidently.\n",
    "\n",
    "Lastly, for later use, let's save the names of just the top seven (7) cities by numbers of complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "1d631691-ecf6-4b06-b040-ccdef6216d7f"
    }
   },
   "outputs": [],
   "source": [
    "TOP_CITIES = list(df_whiny_cities2.head(7)['name'])\n",
    "TOP_CITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "28e3107b-1c28-4cb5-b939-ceb17b2b7797"
    }
   },
   "source": [
    "**Exercise 3** (1 point). Implement a function that takes a list of strings, `str_list`, and returns a single string consisting of each value, `str_list[i]`, enclosed by double-quotes and separated by a comma-space delimiters. For example, if\n",
    "\n",
    "```python\n",
    "   assert str_list == ['a', 'b', 'c', 'd']\n",
    "```\n",
    "\n",
    "then\n",
    "\n",
    "```python\n",
    "   assert strs_to_args(str_list) == '\"a\", \"b\", \"c\", \"d\"'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "dea6e24f95b62c3ff149f8b393f4a55a",
     "grade": false,
     "grade_id": "strs_to_args",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "nbpresent": {
     "id": "c43f2173-4a9f-48c2-8c3f-2a0fafaa8487"
    }
   },
   "outputs": [],
   "source": [
    "def strs_to_args(str_list):\n",
    "    assert type (str_list) is list\n",
    "    assert all ([type (s) is str for s in str_list])\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e95ea71e174e64c255979fb55d6d8c33",
     "grade": true,
     "grade_id": "strs_to_args__test",
     "locked": true,
     "points": 1.0,
     "schema_version": 1,
     "solution": false
    },
    "nbpresent": {
     "id": "8e2b525d-2a5e-466f-928a-d41cc00caa49"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test cell: `strs_to_args__test`\n",
    "\n",
    "print (\"Your solution, applied to TOP_CITIES:\", strs_to_args(TOP_CITIES))\n",
    "\n",
    "TOP_CITIES_as_args = strs_to_args(TOP_CITIES)\n",
    "assert TOP_CITIES_as_args.lower() == \\\n",
    "       '\"BROOKLYN\", \"NEW YORK\", \"BRONX\", \"STATEN ISLAND\", \"Jamaica\", \"Flushing\", \"ASTORIA\"'.lower ()\n",
    "    \n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "3286ec81-ea7b-4693-a5d9-84100e1cf922"
    }
   },
   "source": [
    "**Exercise 4** (3 points). Suppose we want to look at the number of complaints by type _and_ by city. Execute an SQL query to produce a tibble named `df_complaints_by_city` with the variables {`complaint_type`, `city_name`, `complaint_count`}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "b8a2d8ea1ec2ccfd0989eabb3d945a79",
     "grade": false,
     "grade_id": "df_complaints_by_city",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "nbpresent": {
     "id": "2e529060-0407-4e54-871c-e8220c567e7d"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8afaae1992b87951367c4c5e2bbaa789",
     "grade": true,
     "grade_id": "df_complaints_by_city__test",
     "locked": true,
     "points": 3.0,
     "schema_version": 1,
     "solution": false
    },
    "nbpresent": {
     "id": "e025ec55-5163-496d-881d-e35ab0730926"
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `df_complaints_by_city__test`\n",
    "\n",
    "print(\"Reading instructor's solution...\")\n",
    "df_complaints_by_city_soln = pd.read_csv('df_complaints_by_city_soln.csv')\n",
    "\n",
    "print(\"Checking...\")\n",
    "assert tibbles_are_equivalent(df_complaints_by_city,\n",
    "                              df_complaints_by_city_soln)\n",
    "\n",
    "print(\"\\n(Passed.)\")\n",
    "del df_complaints_by_city_soln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "6a9f7704-ce8d-421d-8831-e67a1041a54d"
    }
   },
   "source": [
    "Let's use HoloViews+Bokeh to visualize the results as a stacked bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "9ed520ef-1f82-42c9-9cc1-feba81254403"
    }
   },
   "outputs": [],
   "source": [
    "# Let's consider only the top 25 complaints (by total)\n",
    "top_complaints = df_complaint_freq[:25]\n",
    "print(\"Top complaints:\")\n",
    "display(top_complaints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot subset of data corresponding to the top complaints\n",
    "df_plot = top_complaints.merge(df_complaints_by_city,\n",
    "                               left_on=['type'],\n",
    "                               right_on=['complaint_type'],\n",
    "                               how='left')\n",
    "df_plot.dropna(inplace=True)\n",
    "print(\"Data to plot (first few rows):\")\n",
    "display(df_plot.head())\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this as a stacked bar chart. To do so, we'll need to reshape the data frame so that the values to be stacked appear as columns. It's the perfect application for cast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_plot_stacked = cast(df_plot, key='city_name', value='complaint_count')\n",
    "df_plot_stacked.fillna(0, inplace=True)\n",
    "display(df_plot_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some code to render a Bokeh stacked bar chart\n",
    "\n",
    "kwargs_figure = {'title': \"Distribution of the top 25 complaints among top 7 cities with the most complaints\",\n",
    "                 'width': 800,\n",
    "                 'height': 400,\n",
    "                 'tools': \"hover,crosshair,pan,box_zoom,wheel_zoom,save,reset,help\"}\n",
    "p = make_stacked_bar(df_plot_stacked, 'complaint_type', TOP_CITIES, kwargs_figure=kwargs_figure)\n",
    "\n",
    "p.xaxis.major_label_orientation = 0.66\n",
    "\n",
    "from bokeh.models import HoverTool\n",
    "hover_tool = p.select(dict(type=HoverTool))\n",
    "hover_tool.tooltips = [(\"y\", \"$y{int}\")]\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise 5** (2 points). The preceding code created a dataframe, `df_plot_stacked`, which was then used to create the stacked bar chart shown above.\n",
    "\n",
    "Suppose we want to create a different stacked bar plot that shows, for each complaint type $t$ and city $c$, the fraction of all complaints of type $t$ that occurred in city $c$. Store your result in a dataframe named `df_plot_stacked_fraction`. It should have the same columns as `df_plot_stacked`.\n",
    "\n",
    "> The test cell will create the chart in addition to checking your result. Note that the normalized bars will not necessarily add up to 1; why not?\n",
    "\n",
    "> Note: The normalized bars will not necessarily add up to 1. Why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "a0a4bb0afcdf215db48fcc5cdb917b95",
     "grade": false,
     "grade_id": "norm_above",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "nbpresent": {
     "id": "e8ec1c8f-181e-435b-8c52-73efad01e0df"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "df_plot_stacked_fraction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1b58234dab67368ac2df67bdf105baf2",
     "grade": true,
     "grade_id": "norm_above_test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `norm_above_test`\n",
    "\n",
    "if False:\n",
    "    df_plot_stacked_fraction.to_csv('df_plot_stacked_fraction_soln.csv', index=False)\n",
    "\n",
    "p = make_stacked_bar(df_plot_stacked_fraction, 'complaint_type', TOP_CITIES, kwargs_figure=kwargs_figure)\n",
    "\n",
    "p.xaxis.major_label_orientation = 0.66\n",
    "\n",
    "from bokeh.models import HoverTool\n",
    "hover_tool = p.select(dict(type=HoverTool))\n",
    "hover_tool.tooltips = [(\"y\", \"$y\")]\n",
    "\n",
    "show(p)\n",
    "\n",
    "# Check numerical values\n",
    "df_plot_stacked_fraction_soln = pd.read_csv('df_plot_stacked_fraction_soln.csv')\n",
    "def merge_two_dicts(x, y):\n",
    "    return {**x, **y}\n",
    "\n",
    "def merge_many_dicts(dicts):\n",
    "    x = {}\n",
    "    for d in dicts:\n",
    "        x = merge_two_dicts(x, d)\n",
    "    return x\n",
    "\n",
    "def tuple_sub(a, b):\n",
    "    assert type(a) is tuple and type(b) is tuple and len(a) == len(b)\n",
    "    return tuple(i - j for i, j in zip(a, b))\n",
    "\n",
    "def all_tol(x, tol=1e-14):\n",
    "    return all([abs(i) <= tol for i in x])\n",
    "\n",
    "df_plot_stacked_fraction_soln = pd.read_csv('df_plot_stacked_fraction_soln.csv')\n",
    "row_to_dict = lambda x: {x['complaint_type']: tuple(x[TOP_CITIES])}\n",
    "your_soln = merge_many_dicts(df_plot_stacked_fraction.apply(row_to_dict, axis=1))\n",
    "true_soln = merge_many_dicts(df_plot_stacked_fraction_soln.apply(row_to_dict, axis=1))\n",
    "assert len(your_soln) == len(true_soln)\n",
    "for key_true, value_true in true_soln.items():\n",
    "    assert key_true in your_soln, \"Your solution is missing the complaint type, '{}'\".format(key_true)\n",
    "    value_yours = your_soln[key_true]\n",
    "    assert all_tol(tuple_sub(value_yours, value_true)), \"Data for complaint '{}' of your solution differs from that of reference solution.\".format(key_true)\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "7bf18d88-66cb-449d-8590-a49337d7a7ab"
    }
   },
   "source": [
    "## Dates and times in SQL\n",
    "\n",
    "Recall that the input data had a column with timestamps corresponding to when someone submitted a complaint. Let's quickly summarize some of the features in SQL and Python for reasoning about these timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "37eacdc8-84ad-4d67-8408-9884b7854dbd"
    }
   },
   "source": [
    "The `CreatedDate` column is actually a specially formatted date and time stamp, where you can query against by comparing to strings of the form, `YYYY-MM-DD hh:mm:ss`.\n",
    "\n",
    "For example, let's look for all complaints on September 15, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "cc53c4a5-e993-45ac-8714-a68df87625cd"
    }
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  select ComplaintType, CreatedDate, City\n",
    "    from data\n",
    "    where CreatedDate >= \"2015-09-15 00:00:00.0\"\n",
    "      and CreatedDate < \"2015-09-16 00:00:00.0\"\n",
    "    order by CreatedDate\n",
    "'''\n",
    "df = pd.read_sql_query (query, disk_engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "0d24847f-3ca6-4458-ac02-8fcb70d382e0"
    }
   },
   "source": [
    "This next example shows how to extract just the hour from the time stamp, using SQL's `strftime()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "53acb2a8-c0d6-4f11-8512-5de7404fef2a"
    }
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  select CreatedDate, strftime ('%H', CreatedDate) as Hour, ComplaintType\n",
    "    from data\n",
    "    limit 5\n",
    "'''\n",
    "df = pd.read_sql_query (query, disk_engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "44ed4db0-df35-4547-93aa-5680ad1d5cdf"
    }
   },
   "source": [
    "**Exercise 6** (3 points). Construct a tibble called `df_complaints_by_hour`, which contains the total number of complaints during a given hour of the day. That is, the variables should be {`hour`, `count`} where each observation is the total number of complaints (`count`) that occurred during a given `hour`.\n",
    "\n",
    "> Interpret `hour` as follows: when `hour` is `02`, that corresponds to the open time interval [`02:00:00`, `03:00:00.0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "7b3b054b8357fb223320523164d40c7f",
     "grade": false,
     "grade_id": "df_complaints_by_hour",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "nbpresent": {
     "id": "eb10399f-fc01-4283-b1e5-1817de4b7539"
    }
   },
   "outputs": [],
   "source": [
    "# Your task: Construct `df_complaints_by_hour` as directed.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Displays your answer:\n",
    "display(df_complaints_by_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f55218c9aef4da2e2fbf2adf97885bec",
     "grade": true,
     "grade_id": "df_complaints_by_hour_test",
     "locked": true,
     "points": 3.0,
     "schema_version": 1,
     "solution": false
    },
    "nbpresent": {
     "id": "24e3056d-62c6-4a24-9f3f-b76201cf8754"
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `df_complaints_by_hour_test`\n",
    "    \n",
    "print (\"Reading instructor's solution...\")\n",
    "if False:\n",
    "    df_complaints_by_hour_soln.to_csv('df_complaints_by_hour_soln.csv', index=False)\n",
    "df_complaints_by_hour_soln = pd.read_csv ('df_complaints_by_hour_soln.csv')\n",
    "display (df_complaints_by_hour_soln)\n",
    "\n",
    "df_complaints_by_hour_norm = df_complaints_by_hour.copy ()\n",
    "df_complaints_by_hour_norm['hour'] = \\\n",
    "    df_complaints_by_hour_norm['hour'].apply (int)\n",
    "assert tibbles_are_equivalent (df_complaints_by_hour_norm,\n",
    "                               df_complaints_by_hour_soln)\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the hour-by-hour breakdown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "60bd03aa-679c-4be1-aaed-ec5d22115272"
    }
   },
   "outputs": [],
   "source": [
    "%%opts Bars [width=640 height=320]\n",
    "Bars(df_complaints_by_hour, kdims=['hour'], vdims=['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "674c54f7-dd78-4f01-af25-9bef67a44833"
    }
   },
   "source": [
    "An unusual aspect of these data are the excessively large number of reports associated with hour 0 (midnight up to but excluding 1 am), which would probably strike you as suspicious. Indeed, the reason is that there are some complaints that are dated but with no associated time, which was recorded in the data as exactly `00:00:00.000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "20a45c3b-927f-4085-bcd8-9ee389d78219"
    }
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  select count(*)\n",
    "    from data\n",
    "    where strftime ('%H:%M:%f', CreatedDate) = '00:00:00.000'\n",
    "'''\n",
    "\n",
    "pd.read_sql_query (query, disk_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "e9ec9d63-dcdc-4df7-a0c9-aeefd333cd66"
    }
   },
   "source": [
    "**Exercise 7** (2 points). What is the most common hour for noise complaints? Compute a tibble called `df_noisy_by_hour` whose variables are {`hour`, `count`} and whose observations are the number of noise complaints that occurred during a given `hour`. Consider a \"noise complaint\" to be any complaint string containing the word `noise`. Be sure to filter out any dates _without_ an associated time, i.e., a timestamp of `00:00:00.000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "add192de2e4dc9ec7a06153c42cfd19b",
     "grade": false,
     "grade_id": "df_noisy_by_hour",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "nbpresent": {
     "id": "d61ad646-a163-4de6-a1ff-e3b983f00d99"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "display(df_noisy_by_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "71a2ab4a9fbf439d33686eb94b4d39b2",
     "grade": true,
     "grade_id": "df_noisy_by_hour_test",
     "locked": true,
     "points": 2.0,
     "schema_version": 1,
     "solution": false
    },
    "nbpresent": {
     "id": "2a0c6d57-1438-496d-8ad0-c9e691a2e464"
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `df_noisy_by_hour_test`\n",
    "\n",
    "print (\"Reading instructor's solution...\")\n",
    "if False:\n",
    "    df_noisy_by_hour.to_csv('df_noisy_by_hour_soln.csv', index=False)\n",
    "df_noisy_by_hour_soln = pd.read_csv ('df_noisy_by_hour_soln.csv')\n",
    "display(df_noisy_by_hour_soln)\n",
    "\n",
    "df_noisy_by_hour_norm = df_noisy_by_hour.copy()\n",
    "df_noisy_by_hour_norm['hour'] = \\\n",
    "    df_noisy_by_hour_norm['hour'].apply(int)\n",
    "assert tibbles_are_equivalent (df_noisy_by_hour_norm,\n",
    "                               df_noisy_by_hour_soln)\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "9850fa6b-d951-4f9f-9d23-c8243a75487c"
    }
   },
   "outputs": [],
   "source": [
    "%%opts Bars [width=640 height=320]\n",
    "Bars(df_noisy_by_hour, kdims=['hour'], vdims=['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "5e81087c-d19d-497b-9d5c-1a614527a025"
    }
   },
   "source": [
    "**Exercise 8** (ungraded). Create a line chart to show the fraction of complaints (y-axis) associated with each hour of the day (x-axis), with each complaint type shown as a differently colored line. Show just the top 5 complaints (`top_complaints[:5]`). Remember to exclude complaints with a zero-timestamp (i.e., `00:00:00.000`).\n",
    "\n",
    "> **Note.** This exercise is ungraded but we recommend spending some time giving it a try! Feel free to discuss your approaches to this problem on the discussion forums (but do try to do it yourself first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "07a55d98cb7bc8256dcd3429ce5e5a39",
     "grade": true,
     "grade_id": "complaint_types_by_hour",
     "locked": false,
     "points": 0.0,
     "schema_version": 1,
     "solution": true
    },
    "nbpresent": {
     "id": "396c64c5-fcdb-49dc-9c7b-dddd10cc8d2e"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "5f08feb8-ec73-4c71-9f22-9c9f2d00b418"
    }
   },
   "source": [
    "### Learn more\n",
    "\n",
    "- Find more open data sets on [Data.gov](https://data.gov) and [NYC Open Data](https://nycopendata.socrata.com)\n",
    "- Learn how to setup [MySql with Pandas and Plotly](http://moderndata.plot.ly/graph-data-from-mysql-database-in-python/)\n",
    "- Big data workflows with [HDF5 and Pandas](http://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "nbpresent": {
   "slides": {
    "44693b5c-1b6d-4a8a-93dd-9c739ac5f85b": {
     "id": "44693b5c-1b6d-4a8a-93dd-9c739ac5f85b",
     "prev": null,
     "regions": {
      "f1a0724d-d92a-4c41-8b5c-9d5becd385c6": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0.024495214835408653,
        "y": -0.019794140456002196
       },
       "id": "f1a0724d-d92a-4c41-8b5c-9d5becd385c6"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
